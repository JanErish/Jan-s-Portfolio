{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1><span style=\"font-size:62px;font-family:Times New Roman,Times,serif\"><tt>Credit Card Fraud Detection</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "\n",
    "<h1><span style=\"font-size:20px;font-family:Times New Roman,Times,serif\"><tt>Author: Jan Erish Baluca</tt></span></h1>\n",
    "[**LinkedIn**](https://www.linkedin.com/in/jan-erish-baluca-099569103/)  \n",
    "[**Portfolio on Github**](https://github.com/JanErish/Portfolio_von_Jan)  \n",
    "\n",
    "\n",
    "<a id=\"desc\"></a>\n",
    "\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>Part II</tt></span></h1>\n",
    "</center>\n",
    "\n",
    "![Imgur](https://i.imgur.com/WADp795.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>EXTREME GRADIENT BOOSTING PIPELINE</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "![Imgur](https://i.imgur.com/dwEyicL.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:6px;font-family:Times New Roman,Times,serif\"><tt>image: Shutterstock</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "\n",
    "This section will serve to demonstrate the following skills:\n",
    "1. Object Oriented Programming: Building complex classes and functions\n",
    "2. Data Preprocessing\n",
    "3. Data pipeline (scikit-learn) building\n",
    "4. Model tuning and cross-validation\n",
    "5. XGBoost model creation\n",
    "\n",
    "The end product will be a class or module for easily training XGBoost supervised machine learning models that:\n",
    "* automatically preprocesses data\n",
    "    * dealing with missing values\n",
    "    * encoding categorical variables\n",
    "    * resampling the dataset to deal with class imbalance and skewdness\n",
    "    * normalizing features\n",
    "    * reducing dimensionality\n",
    "    * selecting important features\n",
    "* automatically finds the best setting for a model to deal with a specific dataset through hyperparameter tuning\n",
    "* automatically trains the model using the best parameters found\n",
    "* automatically evaluates model performance through cross-validation\n",
    "\n",
    "The advantages of using the class are:\n",
    "* The end user will not have to backtrack through the source code to change very specific settings.\n",
    "* The end user will not have to copy paste hundreds of lines of code everytime a model has to be trained.\n",
    "* Preprocessing and training settings are all packed in each instance and are easy to access and modify.\n",
    "* The end user will be able to calibrate the same specific configurations for all instances of the XGBoost_trainer with possibly one line, eliminating the need to calibate each one by one.\n",
    "* The trainer is built from the ground up with expert knowledge: best-practices and conventions in the industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1><span style=\"font-size:38px;font-family:Times New Roman,Times,serif\"><tt>!!work in progress!!</tt></span></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the data and basic packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df = df.drop('Time', axis=1)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "index     284807 non-null int64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(29), int64(2)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxcAAAG7CAYAAACxYWiAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZFbcHJBRERERERu8V/GI8bDifF/ogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2795c0727b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Split into train and test sets\n",
    "target_name = 'Class'\n",
    "X = df.drop(target_name, axis =1)\n",
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = pd.DataFrame(df[target_name])\n",
    "\n",
    "# Is it recommended to stratify in order to keep the same proportion of 1s and 0s\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "    #class_weight='balanced')\n",
    "                                    #min_weight_fraction_leaf=0.01)\n",
    "dtree = dtree.fit(X_train,y_train)\n",
    "\n",
    "# Plot!\n",
    "\n",
    "importances = dtree.feature_importances_\n",
    "feat_names = df.drop(['Class'], axis=1).columns\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Which features should we keep?\")\n",
    "plt.bar(range(len(indices)), importances[indices], color='lightblue', align='center')\n",
    "plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n",
    "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical', fontsize=14)\n",
    "plt.xlim([-1, len(indices)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 4 columns):\n",
      "V10    284807 non-null float64\n",
      "V12    284807 non-null float64\n",
      "V14    284807 non-null float64\n",
      "V17    284807 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 8.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Drop unwanted features\n",
    "team = feat_names[np.argsort(importances)[-4:]]\n",
    "X = df[team]\n",
    "print(X.info())\n",
    "\n",
    "# Split into train and test sets again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does not mean sklearn pipeline in the strictiest sense\n",
    "\n",
    "actually a CLASS that contains an sklearn pipeline\n",
    "\n",
    "combine Undersampler and Oversampler in one function using an if statement\n",
    "\n",
    "# Pipeline Steps\n",
    "\n",
    "### add option to input either a whole df or X and y\n",
    "#### maybe also the option to input whole DF and then specify the target variable\n",
    "* train_test_split(stratified) + conversion of sets to DMatrices\n",
    "* transform data to dictionary\n",
    "* Imputer\n",
    "### how about missing values for categorical variables?????\n",
    "* StandardScaler\n",
    "* DictVectorizer\n",
    "* AVOIDING DUMMY VARIABLE TRAP\n",
    "* Function - Resampling (Undersampling or SMOTE)\n",
    "* MAYBE Feature Selection\n",
    "* Train Initial XGBoost Model\n",
    "* Function: Hyperparameter Tuning using just the Training Set\n",
    "    * GridSearchCV / RandomizedSearchCV\n",
    "    * Training a new Model using best hyperparameters (return: tuned_model)\n",
    "* Function: XGBoost Cross-validation\n",
    "\n",
    "### Shorted tuning code\n",
    "* by using nested ifs to determine the function name, parameter name, and parameter value\n",
    "\n",
    "### target variable\n",
    "* target is None, use the last column as y\n",
    "* if it is provided, use that as y and drop it to form X\n",
    "\n",
    "methods for the class:\n",
    ".train()\n",
    ".create()\n",
    ".validate()\n",
    ".train_create_validate()\n",
    ".plot()\n",
    "#### .classvariable_override()\n",
    "* method for forcibly setting the new new cls.parameter given as the new self.parameter\n",
    "#### .tune()\n",
    "* class decorator as a method for changing the default class parameters\n",
    "\n",
    "#### .preprocessor__\n",
    "* attribute for extracting just the preprocessing pipeline\n",
    "\n",
    "#### model evaluation has its own preprocessing pipeline fitted\n",
    "* add an optional parameter in fit_transform for channeling an untouched trained model through the preprocessing pipeline until it hits the cross-validation step\n",
    "\n",
    "# BUILD A FUNCTION FOR VISUALIZING DECISION BOUNDARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JanErish\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, minmax_scale, RobustScaler, \\\n",
    "                                    MaxAbsScaler, QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Positive_count = df.iloc[:,-1].sum()\n",
    "Positive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### define a function for undersampling\n",
    "def Binary_Undersampler(df):\n",
    "    \"\"\"This class is applicable only on data for binary classification.\n",
    "    This function assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "    Positive_count = df.iloc[:,-1].sum()\n",
    "    Positive_indices = np.array(df[df.iloc[:,-1] ==1].index)\n",
    "    Negative_indices = df[df.iloc[:,-1] == 0].index\n",
    "    random_Negative_indices = np.array(np.random.choice(Negative_indices, Positive_count, replace = False))\n",
    "    Undersample_indices = np.concatenate([Positive_indices, random_Negative_indices])\n",
    "    Undersampled_df = df.loc[Undersample_indices,:]\n",
    "    return Undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V10</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265518</th>\n",
       "      <td>-0.157403</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>-0.066555</td>\n",
       "      <td>-0.765670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180305</th>\n",
       "      <td>0.284864</td>\n",
       "      <td>0.325560</td>\n",
       "      <td>0.721068</td>\n",
       "      <td>-0.270842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42664</th>\n",
       "      <td>-2.009561</td>\n",
       "      <td>1.820161</td>\n",
       "      <td>0.122746</td>\n",
       "      <td>-0.724616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198723</th>\n",
       "      <td>1.756121</td>\n",
       "      <td>-0.722450</td>\n",
       "      <td>-0.195288</td>\n",
       "      <td>0.072781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82325</th>\n",
       "      <td>-0.558955</td>\n",
       "      <td>-0.054708</td>\n",
       "      <td>0.134659</td>\n",
       "      <td>-0.399525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             V10       V12       V14       V17\n",
       "265518 -0.157403  0.510277 -0.066555 -0.765670\n",
       "180305  0.284864  0.325560  0.721068 -0.270842\n",
       "42664  -2.009561  1.820161  0.122746 -0.724616\n",
       "198723  1.756121 -0.722450 -0.195288  0.072781\n",
       "82325  -0.558955 -0.054708  0.134659 -0.399525"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost Model Trainer (custom decorator function)\n",
    "\n",
    "def XGBoost_Trainer(df, target=None, model= 'other', resampling=None,\n",
    "                    booster=\"gbtree\", objective=\"reg:linear\",\n",
    "                    test_size=0.2,\n",
    "                    random_state=69,\n",
    "                    normalization=True,\n",
    "                    pca=False,\n",
    "                    hyperparameter_test= \"none\",\n",
    "                    cv_metric=\"rmse\",\n",
    "                    missing_values=\"NaN\",\n",
    "                    imputation_strategy=\"mean\",\n",
    "                    n_iter= 25,\n",
    "                    hypertune_folds= 5,\n",
    "                    cv_folds= 5,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    grid_params = \"preset\",\n",
    "                    random_params = \"preset\"):\n",
    "    \"\"\"This function requires the xgboost package to be imported as xgb.\"\"\"\n",
    "    \n",
    "    if model == \"Tree Regressor\":\n",
    "        bj = ['gbtree','reg:linear']\n",
    "    elif model == \"Linear Regressor\":\n",
    "        bj = ['gblinear','reg:linear']\n",
    "    elif model == \"Tree Classifier\":\n",
    "        bj = ['gbtree', 'reg:logistic']\n",
    "    elif model == \"Linear Classifier\":\n",
    "        bj = ['gblinear', 'reg:logistic']\n",
    "    elif mode == 'other':\n",
    "        bj = [booster, objective]\n",
    "    \n",
    "    grid_parameters = {}\n",
    "    random_parameters = {}\n",
    "    \n",
    "    if grid_params == \"preset\":\n",
    "        grid_params = grid_parameters\n",
    "    if random_params == \"preset\":\n",
    "        random_params == random_parameters\n",
    "    \n",
    "    regression_objectives=[]\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "\n",
    "    # Data Preprocessing Pipeline Steps\n",
    "    preprocessing_steps = [\n",
    "            (\"Imputer\", Imputer(missing_values=missing_values, strategy= imputation_strategy, axis=0))\n",
    "                            ]\n",
    "    \n",
    "    # Scaling\n",
    "    if normalization == True:\n",
    "        preprocessing_steps = preprocessing_steps + [\n",
    "            (\"Scaler\", StandardScaler())\n",
    "                            ]\n",
    "    \n",
    "    # Dummy Variable Creation\n",
    "    class Dictionator:\n",
    "        def __init__(self, orient=\"records\"):\n",
    "            self.orient = orient\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):\n",
    "            return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "    \n",
    "    \n",
    "    preprocessing_steps = preprocessing_steps + [ \n",
    "        (\"Encoder\", FeatureUnion(\n",
    "            [(\"Label\",LabelEncoder()),\n",
    "             (\"Hot\", OneHotEncoder())]))\n",
    "                            ]  \n",
    "    \n",
    "    #### define a CLASS for undersampling\n",
    "    class Binary_Undersampler:\n",
    "        \"\"\"This class is applicable only on data for binary classification.\n",
    "        This class assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):   \n",
    "            self.Positive_count = y.values.sum()\n",
    "            self.Positive_indices = np.array(y[y.iloc[:,-1] ==1].index)\n",
    "            self.Negative_indices = y[y.iloc[:,-1] == 0].index\n",
    "            self.random_Negative_indices = np.array(np.random.choice(self.Negative_indices, self.Positive_count,\n",
    "                                                                     replace = False))\n",
    "            self.Undersample_indices = np.concatenate([self.Positive_indices, self.random_Negative_indices])\n",
    "            return X.loc[self.Undersample_indices,:], y.loc[self.Undersample_indices,:]\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "\n",
    "    \n",
    "    # Resampling Method\n",
    "    if objective in classification_objectives or model in [\"Tree Classifier\",\"Linear Classifier\"]:\n",
    "        if resampling == \"Undersampling\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Undersampler\", Binary_Undersampler())]\n",
    "        elif resampling == \"SMOTE\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Oversampler\", SMOTE())]\n",
    "            \n",
    "    # Principal Component Analysis (PCA)\n",
    "    if pca == True:\n",
    "        preprocessing_steps = preprocessing_steps + [(\"PCA\", PCA())]\n",
    "    \n",
    "    # Separating features and target\n",
    "    X, y = df[df.columns.tolist()[:,-1]], df[df.columns.tolist()[-1]]\n",
    "    \n",
    "    # Split between test and training sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    \n",
    "    # Completing the pipeline with the appropriat XGBoost Model\n",
    "    # correct version\n",
    "    if bj[0] == \"gblinear\":\n",
    "        DM_train = xgb.DMAtrix(data=X_train, label=y_train)\n",
    "        if hyperparameter_test == \"Grid Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", GridSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                      param_grid= grid_params,\n",
    "                                      scoring= scoring,\n",
    "                                      cv= hypertune_folds,\n",
    "                                      verbose=1))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"Random Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", RandomizedSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                                                param_distributions=random_parameters,\n",
    "                                                                scoring= scoring,\n",
    "                                                                cv = hypertune_folds,\n",
    "                                                                n_iter= n_iter))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"none\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Model\", xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "    \n",
    "    elif bj[0] == \"gbtree\":\n",
    "        if bj[1] in regression_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBRegressor(booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "\n",
    "        elif bj[1] in classification_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBClassifier(booster=bj[0], objective=bj[1]))\n",
    "                        ]             \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fitting the pipeline \n",
    "    pipeline.fit(X_train.to_dict(\"records\"), y_train)\n",
    "    \n",
    "    if hyperparameter_test == \"Grid Search\" or hyperparameter_test == \"Random Search\":\n",
    "        if bj[0] == \"gblinear\":\n",
    "            xgb_model = xgb.XGBRegressor(booster=\"gbtree\", objective=\"reg:linear\", **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Linear Regressor\":\n",
    "            xgb_model == xgb.train(booster=\"gblinear\", objective='reg:linear', **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Tree Classifier\":\n",
    "            xgb_model == XGBClassifier(booster=\"gbtree\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imputer class\n",
    "# Imputer(missing_values=missing_values, strategy= imputation_strategy, axis=0)\n",
    "# Imputer = imputer(missing_values = ‘NaN’, strategy = ‘mean’, axis = 0)\n",
    "# Imputer = imputer.fit(x[:,1:3])\n",
    "# X[:,1:3] = imputer.transform(x[:,1:3])\n",
    "\n",
    "\n",
    "class Imputation_Nation:\n",
    "        \"\"\"This class automatically drops rows with missing data for a categorical target/dependent variable\n",
    "        under the assumption that the data is Missing Completely at Random (MCAR)\n",
    "        \n",
    "        This class also handles the vectorization and creation of dummy variables for categorical independent variables.\"\"\"\n",
    "        classification_objectives = [\"reg:logistic\"]\n",
    "        \n",
    "        def __init__(self, missing_values='NaN', num_imput_strat='mean', cat_imput_strat='most_frequent', imputation_axis=0):\n",
    "            self.missing_values = missing_values\n",
    "            self.num_imput_strat = num_imput_strat\n",
    "            self.cat_imput_strat = cat_imput_strat\n",
    "            self.imputation_axis = imputation_axis\n",
    "            \n",
    "        def fit(self, X, y=None, test_model=None):\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X, y=None, test_model=None):\n",
    "            if y is None:\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                \n",
    "            # drop rows that have missing dependent variable values\n",
    "            null_indeces = pd.isnull(y).any(1).nonzero()[0]\n",
    "            X_full = pd.DataFrame(X.drop(X.index[null_indeces]))\n",
    "            y_full = pd.DataFrame(y.drop(y.index[null_indeces]))\n",
    "                \n",
    "            # identify categorical columns\n",
    "            Cat_mask = X_full.dtypes == object\n",
    "            Cat_cols = X_full.columns[Cat_mask].tolist()\n",
    "            # identify numeric columns\n",
    "            Num_cols = X_full.select_dtypes(exclude=['object']).columns.tolist()\n",
    "                    \n",
    "            # Encoding categorical columns\n",
    "            X_full[Cat_cols] = X_full[Cat_cols].apply(lambda x: LabelEncoder().fit_transform(x.astype(str)))\n",
    "                    \n",
    "            # Imputing categorical data\n",
    "            X_full[Cat_cols] = Imputer(missing_values= self.missing_values, strategy=self.cat_imput_strat, \n",
    "                                         axis=self.imputation_axis).fit_transform(X_full[Cat_cols])\n",
    "                    \n",
    "            # Creating dummy variables for categorical data\n",
    "            X_full = OneHotEncoder(categorical_features=Cat_mask, sparse=False).fit_transform(X_full)\n",
    "                    \n",
    "            # Imputing numeric columns\n",
    "            X_full[Num_cols] = Imputer(missing_values=self.missing_values,strategy=self.num_imput_strat, \n",
    "                                axis=self.imputation_axis).fit_transform(X_full[Num_cols])\n",
    "            \n",
    "            if objective in classification_objectives:\n",
    "                # Encoding, Imputing and Creating Dummy Variables for a Categorical Dependent Variable\n",
    "                if y_full.dtypes == object:\n",
    "                    y_full = LabelEncoder().fit_transform(y_full.astype(str))\n",
    "                \n",
    "                y_full = OneHotEncoder(sparse=False).fit_transform(y_full)\n",
    "                \n",
    "                \n",
    "            if test_model is None:\n",
    "                return X_full, y_full\n",
    "            return X_full, y_full, test_model\n",
    "        \n",
    "        def fit_transform(self, X, y=None, test_model=None):\n",
    "            self.fit(X, y, test_model)\n",
    "            # remember if y is None: concatenate X and y\n",
    "            return self.transform(X, y, test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaler\n",
    "\n",
    "# init parameters should be adjustable as soon as the entire class is initiated\n",
    "class Feature_Scaler:\n",
    "    \"\"\"Options for scaling methods (parameter \"scale=\") are the following:\n",
    "        Normal, Standard, MinMax, Normal, Robust, MaxAbs, and Quantile.\n",
    "        \n",
    "        If the fit_transform method is given one positional argument (X), \n",
    "        the class assumes that it is a DataFrame with the last\n",
    "        column as the target variable and the rest as the features.\"\"\"\n",
    "    def __init__(self, scale=\"Standard\", copy=True, with_mean=True, with_std=True,\n",
    "                feature_range=(0, 1),\n",
    "                 norm='l2',\n",
    "                 with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0),\n",
    "                 n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False,\n",
    "                 subsample=100000, random_state=69\n",
    "                ):\n",
    "        if scale == \"Standard\":\n",
    "            self.copy = copy\n",
    "            self.with_mean = with_mean\n",
    "            self.with_std = with_std\n",
    "            self.scale = StandardScaler(copy = self.copy, with_mean = self.with_mean, with_std = self.with_std)\n",
    "        elif scale == \"MinMax\":\n",
    "            self.feature_range= feature_range\n",
    "            self.copy = copy\n",
    "            self.scale = MinMaxScaler(feature_range = self.feature_range, copy = self.copy)\n",
    "        elif scale == \"Normal\":\n",
    "            self.norm = norm\n",
    "            self.copy = copy\n",
    "            self.scale = Normalizer(norm = self.norm, copy = self.copy)\n",
    "        elif scale == \"Robust\":\n",
    "            self.with_centering = with_centering\n",
    "            self.with_scaling = with_scaling\n",
    "            self.quantile_range = quantile_range\n",
    "            self.copy = copy\n",
    "            self.scale = RobustScaler(with_centering = self.with_centering, with_scaling = self.with_scaling,\n",
    "                                     quantile_range=self.quantile_range, copy=self.copy)\n",
    "        elif scale == \"MaxAbs\":\n",
    "            self.copy = copy\n",
    "            self.scale == MaxAbsScaler(copy=self.copy)\n",
    "        elif scale == \"Quantile\":\n",
    "            self.n_quantiles = n_quantiles\n",
    "            self.output_distribution = output_distribution\n",
    "            self.ignore_implicit = ignore_implicit\n",
    "            self.subsample = subsample\n",
    "            self.random_state = random_state\n",
    "            self.copy = copy\n",
    "            self.scale == QuantileTransformer(n_quantiles=self.n_quantiles, output_distribution=self.output_distribution,\n",
    "                                             ignore_implicit=self.ignore_implicit, subsample=self.subsample,\n",
    "                                             random_state=self.random_state)\n",
    "    \n",
    "    def fit(self, X, y=0):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=0):\n",
    "        if y == 0:\n",
    "            return pd.concat([pd.DataFrame(self.scale.fit_transform(X.drop(df.columns[[-1,]], axis=1))), \\\n",
    "                              pd.DataFrame(X.iloc[:,[0]])], \\\n",
    "                        axis=1)\n",
    "        return pd.DataFrame(self.scale.fit_transform(X)), pd.DataFrame(y)\n",
    "    \n",
    "    def fit_transform(self, X, y=0):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Binary_Undersampler:\n",
    "    \"\"\"This class is applicable only on data for binary classification.\n",
    "    This function assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):   \n",
    "        Positive_count = y.values.sum()\n",
    "        Positive_indices = np.array(y[y.iloc[:,-1] ==1].index)\n",
    "        Negative_indices = y[y.iloc[:,-1] == 0].index\n",
    "        random_Negative_indices = np.array(np.random.choice(Negative_indices, Positive_count, \n",
    "                                                                 replace = False))\n",
    "        Undersample_indices = np.concatenate([Positive_indices, random_Negative_indices])\n",
    "        return X.loc[Undersample_indices,:], y.loc[Undersample_indices,:]\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for transforming to dictionary\n",
    "\n",
    "class Dictionator:\n",
    "    def __init__(self, orient=\"records\"):\n",
    "        self.orient = orient\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "# Class for transforming to DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionator:\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "    \n",
    "    def __init__(self, orient=\"records\", objective=\"reg:linear\"):\n",
    "        self.orient = orient\n",
    "        self.objective = objective\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        if self.objective in self.classification_objectives:\n",
    "            return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        return X.to_dict(self.orient), y\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy Variable-creator class\n",
    "class Dummynatrix:\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "        \n",
    "    def __init__(self, sparse=False, objective=\"reg:linear\"):\n",
    "        self.sparse = sparse\n",
    "        self.objective = objective\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y):\n",
    "        if self.objective in self.classification_objectives:\n",
    "            return DictVectorizer(sparse=self.sparse).fit_transform(X), \\\n",
    "                        DictVectorizer(sparse=self.sparse).fit_transform(y)\n",
    "        return DictVectorizer(sparse=self.sparse).fit_transform(X), y\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for transforming to DMatrix\n",
    "class DMatrix_Reloader:\n",
    "    \n",
    "    def fit(self, data, label):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, label):\n",
    "        return xgb.DMatrix(data= data, label= label )\n",
    "    \n",
    "    def fit_transform(self, data, label):\n",
    "        self.fit(data, label)\n",
    "        return self.transform(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    if hyperparameter_test == \"Grid Search\":\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# instantiate pipeline: Tree Booster\n",
    "pipeline = [\n",
    "    (\"Imputer\", Imputer(missing_values=missing_vals, strategy= imput_strat,axis=0)),\n",
    "    (\"Scaler\", StandardScaler()),\n",
    "    (\"Dictifier\", DictVectorizer()),\n",
    "    (\"Tuner\", )\n",
    "            ]\n",
    "\n",
    "    # Convert to dictionaries\n",
    "    X_train, y_train, X_test, y_test = X_train.to_dict('records'),\n",
    "                                        y_train.to_dict('records'),\n",
    "                                        X_test.to_dict('records'),\n",
    "                                        y_test.to_dict('records')\n",
    "\n",
    "                # GBLINEAR INCOMPATIBLE WITH SKLEARN CROSS-VALIDATION\n",
    "                    if hyperparameter_test == \"Grid Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", GridSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                      param_grid= grid_params,\n",
    "                                      scoring= scoring,\n",
    "                                      cv= hypertune_folds,\n",
    "                                      verbose=1))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"Random Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", RandomizedSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                                                param_distributions=random_parameters,\n",
    "                                                                scoring= scoring,\n",
    "                                                                cv = hypertune_folds,\n",
    "                                                                n_iter= n_iter))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"none\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Model\", xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost Model Trainer: IF statement version\n",
    "\n",
    "def XGBTree_Oversample_Trainer(df, metrics=\"rmse\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDERSAMPLED DATA SHOULD NOT OVERWRITE THE ORIGINAL DATA which should probably be used for model evaluation\n",
    "\n",
    "## try testing the model on the negatives from the original that were removed (!= random_Negative_indices)\n",
    "## Try to include (!= random_Negative_indices in the test set ONLY)\n",
    "### pd.concat from the original_df\n",
    "\n",
    "\n",
    "\n",
    "#### Number of data points in the minority class\n",
    "number_records_fraud = len(data[data.Class == 1])\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "#### Picking the indices of the normal classes\n",
    "normal_indices = data[data.Class == 0].index\n",
    "\n",
    "#### Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "#### Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "#### Under sample dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'Class']\n",
    "\n",
    "#### Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COPY\n",
    "# XGBoost Model Trainer (custom decorator function)\n",
    "\n",
    "def XGBoost_Trainer(df, target=None, model= 'other', resampling=None,\n",
    "                    booster=\"gbtree\", objective=\"reg:linear\",\n",
    "                    test_size=0.2,\n",
    "                    random_state=69,\n",
    "                    normalization=True,\n",
    "                    pca=False,\n",
    "                    hyperparameter_test= \"none\",\n",
    "                    cv_metric=\"rmse\",\n",
    "                    missing_values=\"NaN\",\n",
    "                    imputation_strategy=\"mean\",\n",
    "                    n_iter= 25,\n",
    "                    hypertune_folds= 5,\n",
    "                    cv_folds= 5,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    grid_params = \"preset\",\n",
    "                    random_params = \"preset\"):\n",
    "    \"\"\"This function requires the xgboost package to be imported as xgb.\"\"\"\n",
    "    \n",
    "    if model == \"Tree Regressor\":\n",
    "        bj = ['gbtree','reg:linear']\n",
    "    elif model == \"Linear Regressor\":\n",
    "        bj = ['gblinear','reg:linear']\n",
    "    elif model == \"Tree Classifier\":\n",
    "        bj = ['gbtree', 'reg:logistic']\n",
    "    elif model == \"Linear Classifier\":\n",
    "        bj = ['gblinear', 'reg:logistic']\n",
    "    elif mode == 'other':\n",
    "        bj = [booster, objective]\n",
    "    \n",
    "    grid_parameters = {}\n",
    "    random_parameters = {}\n",
    "    \n",
    "    if grid_params == \"preset\":\n",
    "        grid_params = grid_parameters\n",
    "    if random_params == \"preset\":\n",
    "        random_params == random_parameters\n",
    "    \n",
    "    regression_objectives=[]\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "\n",
    "    # Data Preprocessing Pipeline Steps\n",
    "    preprocessing_steps = [\n",
    "            (\"Imputer\", Imputer(missing_values=missing_values, strategy= imputation_strategy, axis=0))\n",
    "                            ]\n",
    "    \n",
    "    # Scaling\n",
    "    if normalization == True:\n",
    "        preprocessing_steps = preprocessing_steps + [\n",
    "            (\"Scaler\", StandardScaler())\n",
    "                            ]\n",
    "    \n",
    "    # Dummy Variable Creation\n",
    "    class Dictionator:\n",
    "        def __init__(self, orient=\"records\"):\n",
    "            self.orient = orient\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):\n",
    "            return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "    \n",
    "    \n",
    "    preprocessing_steps = preprocessing_steps + [ \n",
    "        (\"Encoder\", FeatureUnion(\n",
    "            [(\"Label\",LabelEncoder()),\n",
    "             (\"Hot\", OneHotEncoder())]))\n",
    "                            ]  \n",
    "    \n",
    "    #### define a CLASS for undersampling\n",
    "    class Binary_Undersampler:\n",
    "        \"\"\"This class is applicable only on data for binary classification.\n",
    "        This class assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):   \n",
    "            self.Positive_count = y.values.sum()\n",
    "            self.Positive_indices = np.array(y[y.iloc[:,-1] ==1].index)\n",
    "            self.Negative_indices = y[y.iloc[:,-1] == 0].index\n",
    "            self.random_Negative_indices = np.array(np.random.choice(self.Negative_indices, self.Positive_count,\n",
    "                                                                     replace = False))\n",
    "            self.Undersample_indices = np.concatenate([self.Positive_indices, self.random_Negative_indices])\n",
    "            return X.loc[self.Undersample_indices,:], y.loc[self.Undersample_indices,:]\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "\n",
    "    \n",
    "    # Resampling Method\n",
    "    if objective in classification_objectives or model in [\"Tree Classifier\",\"Linear Classifier\"]:\n",
    "        if resampling == \"Undersampling\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Undersampler\", Binary_Undersampler())]\n",
    "        elif resampling == \"SMOTE\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Oversampler\", SMOTE())]\n",
    "            \n",
    "    # Principal Component Analysis (PCA)\n",
    "    if pca == True:\n",
    "        preprocessing_steps = preprocessing_steps + [(\"PCA\", PCA())]\n",
    "    \n",
    "    # Separating features and target\n",
    "    X, y = df[df.columns.tolist()[:,-1]], df[df.columns.tolist()[-1]]\n",
    "    \n",
    "    # Split between test and training sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    \n",
    "    # Completing the pipeline with the appropriat XGBoost Model\n",
    "    # correct version\n",
    "    if bj[0] == \"gblinear\":\n",
    "        DM_train = xgb.DMAtrix(data=X_train, label=y_train)\n",
    "        if hyperparameter_test == \"Grid Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", GridSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                      param_grid= grid_params,\n",
    "                                      scoring= scoring,\n",
    "                                      cv= hypertune_folds,\n",
    "                                      verbose=1))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"Random Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", RandomizedSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                                                param_distributions=random_parameters,\n",
    "                                                                scoring= scoring,\n",
    "                                                                cv = hypertune_folds,\n",
    "                                                                n_iter= n_iter))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"none\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Model\", xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "    \n",
    "    elif bj[0] == \"gbtree\":\n",
    "        if bj[1] in regression_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBRegressor(booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "\n",
    "        elif bj[1] in classification_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBClassifier(booster=bj[0], objective=bj[1]))\n",
    "                        ]             \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fitting the pipeline \n",
    "    pipeline.fit(X_train.to_dict(\"records\"), y_train)\n",
    "    \n",
    "    if hyperparameter_test == \"Grid Search\" or hyperparameter_test == \"Random Search\":\n",
    "        if bj[0] == \"gblinear\":\n",
    "            xgb_model = xgb.XGBRegressor(booster=\"gbtree\", objective=\"reg:linear\", **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Linear Regressor\":\n",
    "            xgb_model == xgb.train(booster=\"gblinear\", objective='reg:linear', **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Tree Classifier\":\n",
    "            xgb_model == XGBClassifier(booster=\"gbtree\")\n",
    "\n",
    "            \n",
    "                    return pd.concat([pd.DataFrame(Imputer(missing_values= self.missing_values,\n",
    "                                                           strategy=self.num_imput_strat,\\\n",
    "                                   axis=self.imputation_axis).fit_transform(X.drop(df.columns[[-1,]], axis=1)\\\n",
    "                                                    .drop(X.index[pd.isnull(X.iloc[:,-1]).any(1).nonzero()[0]]))), \\\n",
    "                                pd.DataFrame(X.iloc[:,-1]).dropna()], axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
