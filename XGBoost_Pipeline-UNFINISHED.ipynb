{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the data and basic packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# df = pd.read_csv('creditcard.csv')\n",
    "# df = df.drop('Time', axis=1)\n",
    "# df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# df = pd.read_csv('creditcard.csv')\n",
    "# df = df.drop('Time', axis=1)\n",
    "# df.reset_index(inplace=True)\n",
    "\n",
    "# target_name = 'Class'\n",
    "# X = df.drop(target_name, axis =1)\n",
    "# sc_X = StandardScaler()\n",
    "# X = sc_X.fit_transform(X)\n",
    "# y = pd.DataFrame(df[target_name])\n",
    "\n",
    "# # Split into train and test sets\n",
    "# # Is it recommended to stratify in order to keep the same proportion of 1s and 0s\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# dtree = tree.DecisionTreeClassifier()\n",
    "#     #class_weight='balanced')\n",
    "#                                     #min_weight_fraction_leaf=0.01)\n",
    "# dtree = dtree.fit(X_train,y_train)\n",
    "\n",
    "# # Plot!\n",
    "\n",
    "# importances = dtree.feature_importances_\n",
    "# feat_names = df.drop(['Class'], axis=1).columns\n",
    "\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# plt.figure(figsize=(12,6))\n",
    "# plt.title(\"Which features should we keep?\")\n",
    "# plt.bar(range(len(indices)), importances[indices], color='lightblue', align='center')\n",
    "# plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n",
    "# plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical', fontsize=14)\n",
    "# plt.xlim([-1, len(indices)])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does not mean sklearn pipeline in the strictiest sense\n",
    "\n",
    "actually a CLASS that contains an sklearn pipeline\n",
    "\n",
    "combine Undersampler and Oversampler in one function using an if statement\n",
    "\n",
    "# Pipeline Steps\n",
    "\n",
    "### add option to input either a whole df or X and y\n",
    "#### maybe also the option to input whole DF and then specify the target variable\n",
    "* train_test_split(stratified) + conversion of sets to DMatrices\n",
    "* transform data to dictionary\n",
    "* Imputer\n",
    "### how about missing values for categorical variables?????\n",
    "* StandardScaler\n",
    "* DictVectorizer\n",
    "* AVOIDING DUMMY VARIABLE TRAP\n",
    "* Function - Resampling (Undersampling or SMOTE)\n",
    "* MAYBE Feature Selection\n",
    "* Train Initial XGBoost Model\n",
    "* Function: Hyperparameter Tuning using just the Training Set\n",
    "    * GridSearchCV / RandomizedSearchCV\n",
    "    * Training a new Model using best hyperparameters (return: tuned_model)\n",
    "* Function: XGBoost Cross-validation\n",
    "\n",
    "### Shorted tuning code\n",
    "* by using nested ifs to determine the function name, parameter name, and parameter value\n",
    "\n",
    "### target variable\n",
    "* target is None, use the last column as y\n",
    "* if it is provided, use that as y and drop it to form X\n",
    "\n",
    "methods for the class:\n",
    ".train()\n",
    ".create()\n",
    ".validate()\n",
    ".train_create_validate()\n",
    ".plot()\n",
    "#### .classvariable_override()\n",
    "* method for forcibly setting the new new cls.parameter given as the new self.parameter\n",
    "#### .tune()\n",
    "* class decorator as a method for changing the default class parameters\n",
    "\n",
    "#### .preprocessor__\n",
    "* attribute for extracting just the preprocessing pipeline\n",
    "\n",
    "#### model evaluation has its own preprocessing pipeline fitted\n",
    "* add an optional parameter in fit_transform for channeling an untouched trained model through the preprocessing pipeline until it hits the cross-validation step\n",
    "\n",
    "## .linear_learn()\n",
    "* for training linear base model with CV\n",
    "\n",
    "## early stopping!!!!!!\n",
    "\n",
    "\n",
    "# BUILD A FUNCTION FOR VISUALIZING DECISION BOUNDARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JanErish\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, hamming_loss,\\\n",
    "jaccard_similarity_score, log_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss,\\\n",
    "explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, minmax_scale, RobustScaler, \\\n",
    "                                    MaxAbsScaler, QuantileTransformer\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### define a function for undersampling\n",
    "# def Binary_Undersampler(df):\n",
    "#     \"\"\"This class is applicable only on data for binary classification.\n",
    "#     This function assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "#     Positive_count = df.iloc[:,-1].sum()\n",
    "#     Positive_indices = np.array(df[df.iloc[:,-1] ==1].index)\n",
    "#     Negative_indices = df[df.iloc[:,-1] == 0].index\n",
    "#     random_Negative_indices = np.array(np.random.choice(Negative_indices, Positive_count, replace = False))\n",
    "#     Undersample_indices = np.concatenate([Positive_indices, random_Negative_indices])\n",
    "#     Undersampled_df = df.loc[Undersample_indices,:]\n",
    "#     return Undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost Model Trainer (custom decorator function)\n",
    "\n",
    "def XGBoost_Trainer(df, target=None, model= 'other', resampling=None,\n",
    "                    booster=\"gbtree\", objective=\"reg:linear\",\n",
    "                    test_size=0.2,\n",
    "                    random_state=69,\n",
    "                    normalization=True,\n",
    "                    pca=False,\n",
    "                    hyperparameter_test= \"none\",\n",
    "                    cv_metric=\"rmse\",\n",
    "                    missing_values=\"NaN\",\n",
    "                    imputation_strategy=\"mean\",\n",
    "                    n_iter= 25,\n",
    "                    hypertune_folds= 5,\n",
    "                    cv_folds= 5,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    grid_params = \"preset\",\n",
    "                    random_params = \"preset\"):\n",
    "    \"\"\"This function requires the xgboost package to be imported as xgb.\"\"\"\n",
    "    \n",
    "    if model == \"Tree Regressor\":\n",
    "        bj = ['gbtree','reg:linear']\n",
    "    elif model == \"Linear Regressor\":\n",
    "        bj = ['gblinear','reg:linear']\n",
    "    elif model == \"Tree Classifier\":\n",
    "        bj = ['gbtree', 'reg:logistic']\n",
    "    elif model == \"Linear Classifier\":\n",
    "        bj = ['gblinear', 'reg:logistic']\n",
    "    elif mode == 'other':\n",
    "        bj = [booster, objective]\n",
    "    \n",
    "    grid_parameters = {}\n",
    "    random_parameters = {}\n",
    "    \n",
    "    if grid_params == \"preset\":\n",
    "        grid_params = grid_parameters\n",
    "    if random_params == \"preset\":\n",
    "        random_params == random_parameters\n",
    "    \n",
    "    regression_objectives=[]\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "\n",
    "    # Data Preprocessing Pipeline Steps\n",
    "    preprocessing_steps = [\n",
    "            (\"Imputer\", Imputer(missing_values=missing_values, strategy= imputation_strategy, axis=0))\n",
    "                            ]\n",
    "    \n",
    "    # Scaling\n",
    "    if normalization == True:\n",
    "        preprocessing_steps = preprocessing_steps + [\n",
    "            (\"Scaler\", StandardScaler())\n",
    "                            ]\n",
    "    \n",
    "    # Dummy Variable Creation\n",
    "    class Dictionator:\n",
    "        def __init__(self, orient=\"records\"):\n",
    "            self.orient = orient\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):\n",
    "            return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "    \n",
    "    \n",
    "    preprocessing_steps = preprocessing_steps + [ \n",
    "        (\"Encoder\", FeatureUnion(\n",
    "            [(\"Label\",LabelEncoder()),\n",
    "             (\"Hot\", OneHotEncoder())]))\n",
    "                            ]  \n",
    "    \n",
    "    #### define a CLASS for undersampling\n",
    "    class Binary_Undersampler:\n",
    "        \"\"\"This class is applicable only on data for binary classification.\n",
    "        This class assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):   \n",
    "            self.Positive_count = y.values.sum()\n",
    "            self.Positive_indices = np.array(y[y.iloc[:,-1] ==1].index)\n",
    "            self.Negative_indices = y[y.iloc[:,-1] == 0].index\n",
    "            self.random_Negative_indices = np.array(np.random.choice(self.Negative_indices, self.Positive_count,\n",
    "                                                                     replace = False))\n",
    "            self.Undersample_indices = np.concatenate([self.Positive_indices, self.random_Negative_indices])\n",
    "            return X.loc[self.Undersample_indices,:], y.loc[self.Undersample_indices,:]\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "\n",
    "    \n",
    "    # Resampling Method\n",
    "    if objective in classification_objectives or model in [\"Tree Classifier\",\"Linear Classifier\"]:\n",
    "        if resampling == \"Undersampling\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Undersampler\", Binary_Undersampler())]\n",
    "        elif resampling == \"SMOTE\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Oversampler\", SMOTE())]\n",
    "            \n",
    "    # Principal Component Analysis (PCA)\n",
    "    if pca == True:\n",
    "        preprocessing_steps = preprocessing_steps + [(\"PCA\", PCA())]\n",
    "    \n",
    "    # Separating features and target\n",
    "    X, y = df[df.columns.tolist()[:,-1]], df[df.columns.tolist()[-1]]\n",
    "    \n",
    "    # Split between test and training sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    \n",
    "    # Completing the pipeline with the appropriat XGBoost Model\n",
    "    # correct version\n",
    "    if bj[0] == \"gblinear\":\n",
    "        DM_train = xgb.DMAtrix(data=X_train, label=y_train)\n",
    "        if hyperparameter_test == \"Grid Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", GridSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                      param_grid= grid_params,\n",
    "                                      scoring= scoring,\n",
    "                                      cv= hypertune_folds,\n",
    "                                      verbose=1))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"Random Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", RandomizedSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                                                param_distributions=random_parameters,\n",
    "                                                                scoring= scoring,\n",
    "                                                                cv = hypertune_folds,\n",
    "                                                                n_iter= n_iter))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"none\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Model\", xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "    \n",
    "    elif bj[0] == \"gbtree\":\n",
    "        if bj[1] in regression_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBRegressor(booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "\n",
    "        elif bj[1] in classification_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBClassifier(booster=bj[0], objective=bj[1]))\n",
    "                        ]             \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fitting the pipeline \n",
    "    pipeline.fit(X_train.to_dict(\"records\"), y_train)\n",
    "    \n",
    "    if hyperparameter_test == \"Grid Search\" or hyperparameter_test == \"Random Search\":\n",
    "        if bj[0] == \"gblinear\":\n",
    "            xgb_model = xgb.XGBRegressor(booster=\"gbtree\", objective=\"reg:linear\", **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Linear Regressor\":\n",
    "            xgb_model == xgb.train(booster=\"gblinear\", objective='reg:linear', **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Tree Classifier\":\n",
    "            xgb_model == XGBClassifier(booster=\"gbtree\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDIT SO THAT COLUMN NAMES WILL BE PRESERVED EVEN AFTER IMPUTATION_NATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer Class\n",
    "\n",
    "class Imputation_Nation:\n",
    "        \"\"\"This class handles the imputation of missing values\n",
    "        and the encoding and creation of dummy variables for categorical variables.\n",
    "\n",
    "        If the target variable is passed as y, it will also be imputed if impute_y= True.\n",
    "        Otherwise, y will pass through as untouched.\"\"\"\n",
    "        \n",
    "        classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                            'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "        \n",
    "        missing_values = None\n",
    "        num_imput_strat = None\n",
    "        cat_imput_strat = None\n",
    "        imputation_axis = None\n",
    "        objective = None\n",
    "        impute_y = None\n",
    "        Cat_mask = None\n",
    "        Num_cols = None\n",
    "        \n",
    "        def __init__(self, impute_y=False, objective=\"reg:linear\",missing_values='NaN', num_imput_strat='mean', \n",
    "                     cat_imput_strat='most_frequent', imputation_axis=0,\n",
    "                    Cat_mask = None, Num_cols = None,\n",
    "                    \n",
    "                     classification_objectives=None):\n",
    "        \n",
    "            # assigning parameters as instance variables\n",
    "            varses = list(vars(Imputation_Nation).keys())\n",
    "            self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "            class_name = \"Imputation_Nation\"+\".\"\n",
    "            for v in self.variables:\n",
    "                # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "                if eval(\"%s != None\" % (class_name+v)) is True:\n",
    "                    exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "                # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "                else:\n",
    "                    exec(\"self.%s = %s\" % (v, v)) \n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X, y=None):\n",
    "                \n",
    "            X_full = pd.DataFrame(X)\n",
    "            if y is not None:\n",
    "                y_full = pd.DataFrame(y)\n",
    "                \n",
    "            # identify categorical columns\n",
    "            if self.Cat_mask is None:\n",
    "                Cat_mask = X_full.dtypes == object\n",
    "                Cat_cols = X_full.columns[Cat_mask].tolist()\n",
    "            else:\n",
    "                Cat_mask = self.Cat_mask\n",
    "                Cat_cols = X_full.columns[Cat_mask].tolist()\n",
    "            \n",
    "            # identify numeric columns\n",
    "            if self.Num_cols is None:\n",
    "                Num_cols = X_full.select_dtypes(exclude=['object']).columns.tolist()\n",
    "            else:\n",
    "                Num_cols = self.Num_cols\n",
    "            \n",
    "            # Encoding and imputing categorical variables\n",
    "            if len(Cat_cols) != 0:\n",
    "                for col in X_full.loc[:,Cat_mask].columns:\n",
    "                    L_E = LabelEncoder()\n",
    "                    IMP = Imputer(missing_values= 'NaN', strategy='most_frequent', axis=0)\n",
    "                    le = L_E.fit(X_full.loc[:,col])\n",
    "                    X_full.loc[:,col] = le.transform(X_full.loc[:,col])\n",
    "                    IMP = IMP.fit(X_full.loc[:,col].values.reshape(-1, 1))\n",
    "                    X_full.loc[:,col] = IMP.transform(X_full.loc[:,col].values.reshape(-1, 1)).astype(int)\n",
    "                    X_full.loc[:,col] = le.inverse_transform(X_full.loc[:,col])\n",
    "                    \n",
    "                # Creating dummy variables for categorical data\n",
    "                #X_full = OneHotEncoder(categorical_features=Cat_mask, sparse=False).fit_transform(X_full)\n",
    "                X_full = pd.get_dummies(X_full, drop_first=True, columns=Cat_cols)\n",
    "            \n",
    "            # Imputing numeric columns\n",
    "            if len(Num_cols) != 0:\n",
    "                X_full[Num_cols] = Imputer(missing_values=self.missing_values,strategy=self.num_imput_strat, \n",
    "                                axis=self.imputation_axis).fit_transform(X_full[Num_cols])\n",
    "            \n",
    "            if y is not None and self.impute_y == True:\n",
    "                if self.objective in self.classification_objectives:\n",
    "                    \n",
    "                    # Encoding target variables formatted as string\n",
    "                    if y_full.iloc[:,0].dtype == object:\n",
    "                        Target_is_string = True\n",
    "                    else:\n",
    "                        Target_is_string = False\n",
    "                    \n",
    "                    if Target_is_string == True:\n",
    "                        L_E = LabelEncoder()\n",
    "                        L_E = L_E.fit(y_full.iloc[:,0].astype(str))\n",
    "                        y_full.iloc[:,0] = L_E.transform(y_full.iloc[:,0].astype(str))\n",
    "                    \n",
    "                    IMP = Imputer(missing_values= 'NaN', strategy='most_frequent', axis=0)\n",
    "                    MP = IMP.fit(y_full.iloc[:,0].values.reshape(-1, 1))\n",
    "                    y_full.iloc[:,0] = IMP.transform(y_full.iloc[:,0].values.reshape(-1, 1)).astype(int)\n",
    "                    \n",
    "                    # If target was originally string, return to original string values\n",
    "                    if Target_is_string == True:\n",
    "                        y_full.iloc[:,0] = L_E.inverse_transform(y_full.iloc[:,0])\n",
    "                    \n",
    "                    # Creating dummy variables\n",
    "                    # y_full = pd.get_dummies(y_full, drop_first=True, columns=[y_full.columns[0]])\n",
    "                    # y_full = y_full.astype(int)\n",
    "                    # Create a dictionary map for encoded labels for dependent variable\n",
    "                    # self.target_mapping = dict(zip(L_E.classes_, L_E.transform(L_E.classes_)))\n",
    "                \n",
    "                else:\n",
    "                    y_full.iloc[:,0] = Imputer(missing_values=self.missing_values,strategy=self.num_imput_strat, \n",
    "                                axis=self.imputation_axis).fit_transform(y_full.iloc[:,0])\n",
    "                    \n",
    "            if y is not None:\n",
    "                return pd.DataFrame(X_full), pd.DataFrame(y_full)\n",
    "            return pd.DataFrame(X_full)\n",
    "        \n",
    "        def fit_transform(self, X, y=None):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nullifier:\n",
    "\n",
    "    def find(self, a, valid_values):\n",
    "        a = pd.DataFrame(a) \n",
    "        valid_indeces = a.iloc[:,0].isin(valid_values)\n",
    "        if len(valid_indeces) != 0:\n",
    "            invalid_a = pd.DataFrame(a.drop(a.index[valid_indeces]))\n",
    "            self.null_values = list(invalid_a.iloc[:,0].unique())\n",
    "    \n",
    "    def finder(self, a, valid_values):\n",
    "        self.find(a, valid_values)\n",
    "        return self.null_values\n",
    "    \n",
    "    def nullify(self, a):\n",
    "        a = pd.DataFrame(a)\n",
    "        if len(self.null_values) != 0:\n",
    "            for null_val in self.null_values:\n",
    "                a = a.replace([null_val], np.nan)\n",
    "        return a\n",
    "    \n",
    "    def find_nullify(self, a, valid_values):\n",
    "        self.find(a, valid_values)\n",
    "        return self.nullify(a)\n",
    "    \n",
    "    # DataFrame nullifier\n",
    "    def df_nullifier(self, df, valid_dictionary):\n",
    "        \"\"\" This method should be given a pandas DataFrame and a dictionary comprising of the following key/value pairs:\n",
    "            Key =  column name\n",
    "            Value = list of valid values for the column.\"\"\"\n",
    "        for col, valid_vals in valid_dictionary.items():\n",
    "            df[col] = self.find_nullify(df[col], valid_vals)\n",
    "        return df\n",
    "    \n",
    "    def df_null_dictionary(self, df, valid_dictionary):\n",
    "        \"\"\"Returns a dictionary of values considered null for each DatFrame column\"\"\"\n",
    "        self.null_dictionary = {}\n",
    "        for col, valid_vals in valid_dictionary.items():\n",
    "            self.null_dictionary[col] = self.finder(df[col], valid_vals)\n",
    "        return self.null_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCAR_Dropper:\n",
    "    \"\"\"This transformer is not compatible with sklearn Pipeline.\n",
    "    \n",
    "    This class automatically drops rows with missing data for a categorical target/dependent variable\n",
    "    under the assumption that the data is Missing Completely at Random (MCAR).\n",
    "    \n",
    "    Parameter Null_values can be fed a list of label values that will be considered Null and thus will be dropped\"\"\"\n",
    "    def __init__(self, null_values=None):\n",
    "        self.null_values = null_values\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X, y):\n",
    "        X_nonull = X\n",
    "        y_nonull = y\n",
    "        null_inds = pd.isnull(y).any(1).nonzero()[0]\n",
    "        if len(null_inds) != 0:\n",
    "            X_nonull = pd.DataFrame(X.drop(X.index[null_inds])).reset_index(drop=True)\n",
    "            y_nonull = pd.DataFrame(y.drop(y.index[null_inds])).reset_index(drop=True)\n",
    "        \n",
    "        # drop values considered as null based on given list\n",
    "        if self.null_values is not None:\n",
    "            for value in self.null_values:\n",
    "                null_inds = y_nonull.iloc[:,0] == value\n",
    "                if len(null_inds) != 0:\n",
    "                    X_nonull = pd.DataFrame(X_nonull.drop(X_nonull.index[null_inds])).reset_index(drop=True)\n",
    "                    y_nonull = pd.DataFrame(y_nonull.drop(y_nonull.index[null_inds])).reset_index(drop=True)\n",
    "        \n",
    "        return X_nonull, y_nonull\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ORIGINAL FEATURE SCALER CLASS\n",
    "class Feature_Scaler:\n",
    "    \"\"\"Options for scaling methods (parameter \"scale=\") are the following:\n",
    "        Normal, Standard, MinMax, Normal, Robust, MaxAbs, and Quantile.\n",
    "        \n",
    "            If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    def __init__(self, scale=\"Standard\", copy=True, with_mean=True, with_std=True,\n",
    "                feature_range=(0, 1),\n",
    "                 norm='l2',\n",
    "                 with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0),\n",
    "                 n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False,\n",
    "                 subsample=100000, random_state=69\n",
    "                ):\n",
    "        self.scale = scale\n",
    "        \n",
    "        if self.scale == \"Standard\":\n",
    "            self.copy = copy\n",
    "            self.with_mean = with_mean\n",
    "            self.with_std = with_std\n",
    "            self.scaler = StandardScaler(copy = self.copy, with_mean = self.with_mean, with_std = self.with_std)\n",
    "        elif self.scale == \"MinMax\":\n",
    "            self.feature_range= feature_range\n",
    "            self.copy = copy\n",
    "            self.scaler = MinMaxScaler(feature_range = self.feature_range, copy = self.copy)\n",
    "        elif self.scale == \"Normal\":\n",
    "            self.norm = norm\n",
    "            self.copy = copy\n",
    "            self.scaler = Normalizer(norm = self.norm, copy = self.copy)\n",
    "        elif self.scale == \"Robust\":\n",
    "            self.with_centering = with_centering\n",
    "            self.with_scaling = with_scaling\n",
    "            self.quantile_range = quantile_range\n",
    "            self.copy = copy\n",
    "            self.scaler = RobustScaler(with_centering = self.with_centering, with_scaling = self.with_scaling,\n",
    "                                     quantile_range=self.quantile_range, copy=self.copy)\n",
    "        elif self.scale == \"MaxAbs\":\n",
    "            self.copy = copy\n",
    "            self.scaler == MaxAbsScaler(copy=self.copy)\n",
    "        elif self.scale == \"Quantile\":\n",
    "            self.n_quantiles = n_quantiles\n",
    "            self.output_distribution = output_distribution\n",
    "            self.ignore_implicit = ignore_implicit\n",
    "            self.subsample = subsample\n",
    "            self.random_state = random_state\n",
    "            self.copy = copy\n",
    "            self.scaler == QuantileTransformer(n_quantiles=self.n_quantiles, output_distribution=self.output_distribution,\n",
    "                                             ignore_implicit=self.ignore_implicit, subsample=self.subsample,\n",
    "                                             random_state=self.random_state)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "  \n",
    "        X_scaled = pd.DataFrame(self.scaler.fit_transform(X), columns=X.columns)\n",
    "        \n",
    "        if y is not None:\n",
    "            return X_scaled, pd.DataFrame(y)\n",
    "        return X_scaled\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERSCALER TUNING @@@@@@@@@@@@\n",
    "\n",
    "# Feature_Hyperscaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaler\n",
    "\n",
    "class Feature_Hyperscaler:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for scaling methods (parameter \"scale=\") are the following:\n",
    "        Normal, Standard, MinMax, Normal, Robust, MaxAbs, and Quantile.\n",
    "        \n",
    "        If scale = 'Hyperscale',\n",
    "            every scaling method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The scaling method that contributes to the best score will be applied to the data to be returned.\n",
    "        \n",
    "        This feature selection transformer works only with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        Tuning requires the target (y) to also be passed.\n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    # Compatible model evaluation metrics: Sklearn metrics\n",
    "    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,\n",
    "        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,\n",
    "                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,\n",
    "                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,\n",
    "                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,\n",
    "                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}\n",
    "\n",
    "    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']\n",
    "    \n",
    "    # metrics with which higher value = higher model performance\n",
    "    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',\n",
    "                                  'precision_score','recall_score',\n",
    "                                'explained_variance_score','r2_score','Average test auc',\n",
    "                                'Average test ndcg','Average test map']\n",
    "    # metrics with which lower value = higher model performance\n",
    "    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',\n",
    "                           'mean_squared_error','mean_squared_log_error','median_absolute_error',\n",
    "                           'Average test error','Average test rmse','Average test mae','Average test log loss',\n",
    "                           'Average test merror','Average test mlogloss']\n",
    "\n",
    "    # parameters to inherited from mother class\n",
    "    scale=None\n",
    "    best_params=None\n",
    "    booster=None\n",
    "    objective=None\n",
    "    scorer=None\n",
    "    copy=None\n",
    "    with_mean=None\n",
    "    with_std=None\n",
    "    feature_range=None\n",
    "    norm=None\n",
    "    with_centering=None\n",
    "    with_scaling=None\n",
    "    quantile_range=None\n",
    "    n_quantiles=None\n",
    "    output_distribution=None\n",
    "    ignore_implicit_zeros=None\n",
    "    subsample=None\n",
    "    random_state=None\n",
    "    test_size=None\n",
    "    \n",
    "    def __init__(self, scale=\"Hyperscale\", best_params={},\n",
    "                 booster='gbtree', objective='reg:logistic', scorer='accuracy_score',\n",
    "                 copy=True, with_mean=True, with_std=True,\n",
    "                feature_range=(0, 1),\n",
    "                 norm='l2',\n",
    "                 with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0),\n",
    "                 n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False,\n",
    "                 subsample=100000, random_state=69, test_size=0.2,\n",
    "                # values passed for these parameters will never be used unless: class.parameter is made None\n",
    "                scoring_functions=None, classification_objectives=None,\n",
    "                 the_higher_the_better=None, the_lower_the_better=None):\n",
    "        \n",
    "        # assigning parameters as instance variables\n",
    "        varses = list(vars(Feature_Hyperscaler).keys())\n",
    "        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "        class_name = \"Feature_Hyperscaler\"+\".\"\n",
    "        for v in self.variables:\n",
    "            # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "            if eval(\"%s != None\" % (class_name+v)) is True:\n",
    "                exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "            # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "            else:\n",
    "                exec(\"self.%s = %s\" % (v, v))\n",
    "        \n",
    "        # determining the model build used for feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            if self.objective in Feature_Hyperscaler.classification_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBClassifier(**self.best_params)\n",
    "            elif self.objective in Feature_Hyperscaler.regression_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "            self.best_params['booster'] = self.booster\n",
    "            self.best_params['objective'] = self.objective\n",
    "        \n",
    "        self.scaler_params={'StandardScaler':[StandardScaler,{'copy':self.copy, \n",
    "                                              'with_mean':self.with_mean, 'with_std':self.with_std}],\n",
    "                          'MinMaxScaler':[MinMaxScaler,{'feature_range':self.feature_range, 'copy':self.copy}],\n",
    "                          'Normalizer':[Normalizer,{'norm':self.norm, 'copy':self.copy}],\n",
    "                            'RobustScaler':[RobustScaler,{'with_centering':self.with_centering,\n",
    "                                            'with_scaling':self.with_scaling,\n",
    "                                             'quantile_range':self.quantile_range, 'copy':self.copy}],\n",
    "                           'MaxAbsScaler':[MaxAbsScaler,{'copy':self.copy}],\n",
    "                           'QuantileTransformer':[QuantileTransformer,{'n_quantiles':self.n_quantiles, \n",
    "                                                  'output_distribution':self.output_distribution,\n",
    "                                                  'ignore_implicit_zeros':self.ignore_implicit_zeros,\n",
    "                                                  'subsample':self.subsample,'random_state':self.random_state}]}\n",
    "        \n",
    "        # Assigning the scaler if scale is not \"Hyperscale\"\n",
    "        if self.scale is not 'Hyperscale':\n",
    "            self.scaler = self.scaler_params[self.scale][0](**self.scaler_params[self.scale][1])\n",
    "    \n",
    "    def fit(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if self.target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[self.target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        \n",
    "        if self.scale == 'Hyperscale':\n",
    "            # Encode labels if string when needed\n",
    "            if self.booster == 'dart' and self.objective in Feature_Selector.classification_objectives\\\n",
    "                                                                    or self.scorer == 'f1_score':\n",
    "                if y.iloc[:,0].dtype == object:\n",
    "                    Target_is_string = True\n",
    "                else:\n",
    "                    Target_is_string = False\n",
    "\n",
    "                if Target_is_string == True:\n",
    "                    L_E = LabelEncoder()\n",
    "                    L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "                    y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "\n",
    "            # fit model on all training data            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "            \n",
    "            scales = ['None','StandardScaler','MinMaxScaler','Normalizer',\n",
    "                      'RobustScaler','MaxAbsScaler','QuantileTransformer']\n",
    "            Scores = []\n",
    "            for scale in scales:\n",
    "                if scale is 'None':\n",
    "                    X_train_scaled = X_train\n",
    "                    X_test_scaled = X_test\n",
    "                else:\n",
    "                    scaler = self.scaler_params[scale][0](**self.scaler_params[scale][1])\n",
    "                    X_train_scaled = scaler.fit_transform(X_train)\n",
    "                    X_test_scaled = scaler.fit_transform(X_test)\n",
    "                \n",
    "                if self.booster is 'gbtree':\n",
    "                    if self.objective in Feature_Hyperscaler.classification_objectives:\n",
    "                        estimator = xgb.XGBClassifier\n",
    "                    if self.objective in Feature_Hyperscaler.regression_objectives:\n",
    "                        estimator = xgb.Regressor\n",
    "                    selection_model = estimator(**self.best_params)\n",
    "                    selection_model.fit(X_train_scaled, y_train.values.ravel(),\n",
    "                                       eval_set=[(X_train_scaled,y_train.values.ravel()),\n",
    "                                                 (X_test_scaled,y_test.values.ravel())], verbose=False)\n",
    "                    # eval model\n",
    "                    # using build-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = selection_model.evals_result()\n",
    "                        scorer_used = list(result['validation_1'].keys())[0]\n",
    "                        score = np.mean(result['validation_1'][scorer_used])\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = Feature_Hyperscaler.scoring_functions[self.scorer]\n",
    "                        y_pred = selection_model.predict(X_test_scaled)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "                        \n",
    "                elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "                    dtrain_scaled = xgb.DMatrix(data=X_train_scaled, label=y_train)\n",
    "                    dtest_scaled = xgb.DMatrix(data=X_test_scaled, label=y_test)\n",
    "\n",
    "                    # eval model\n",
    "                    # using build-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = {}\n",
    "                        selection_model = xgb.train(dtrain=dtrain_scaled, params=self.best_params,\n",
    "                                                    evals=[(dtest_scaled, 'eval')], evals_result=result,\n",
    "                                                   verbose_eval=False)\n",
    "                        scorer_used = list(result['eval'].keys())[0]\n",
    "                        score = np.mean(result['eval'][scorer_used])\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = Feature_Hyperscaler.scoring_functions[self.scorer]\n",
    "\n",
    "                        selection_model = xgb.train(dtrain=dtrain_scaled, params=self.best_params)\n",
    "                        y_pred = selection_model.predict(dtest_scaled)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "\n",
    "            # building table of performance scores\n",
    "            if self.scorer == 'Auto':\n",
    "                self.scorer = 'Average test ' + scorer_used\n",
    "            self.performance_scores = pd.DataFrame()\n",
    "            self.performance_scores['Scalers'] = scales\n",
    "            self.performance_scores[self.scorer] = Scores\n",
    "\n",
    "            # Best scaler: returns max possible model performance \n",
    "            if self.scorer in Feature_Selector.the_higher_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == max(self.performance_scores[self.scorer])]\n",
    "            elif self.scorer in Feature_Selector.the_lower_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == min(self.performance_scores[self.scorer])]\n",
    "                \n",
    "            # assigning the best-performing scaler as scaler\n",
    "            self.best_scaler = self.best_perf.iloc[0,0]\n",
    "            if self.best_scaler is not 'None':\n",
    "                self.scaler = self.scaler_params[self.best_scaler][0](**self.scaler_params[self.best_scaler][1])\n",
    "                # Fitting the scaler\n",
    "                self.scaler.fit(X)\n",
    "            else:\n",
    "                self.scaler = None\n",
    "            \n",
    "            # reverse label encoding\n",
    "            if self.booster == 'dart' and self.objective in Feature_Selector.classification_objectives:\n",
    "                if Target_is_string == True:\n",
    "                    y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        else:\n",
    "            # Fitting the scaler\n",
    "            self.scaler.fit(X)\n",
    "            return self\n",
    "    \n",
    "    def transform(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        if self.best_scaler is not 'None':\n",
    "            X_scaled = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "            return X_scaled, pd.DataFrame(y)\n",
    "        else:\n",
    "            return pd.DataFrame(X), pd.DataFrame(y)\n",
    "        \n",
    "    def fit_transform(self, X, y=None, target=None):\n",
    "        self.fit(X,y, target=target)\n",
    "        return self.transform(X, y, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclr = Feature_Hyperscaler(scale='StandardScaler')\n",
    "sclr.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create the DMatrix: housing_dmatrix\n",
    "# dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# # Create the parameter dictionary: params\n",
    "# params = {\"booster\": \"gblinear\", objective\":\"reg:linear\",\"max_depth\":4}\n",
    "\n",
    "# # Train the model: xg_reg\n",
    "# xg_reg = xgb.train(dtrain=dmatrix, params=params, num_boost_round=10)\n",
    "\n",
    "# # Plot the feature importances\n",
    "# xgb.plot_importance(xg_reg)\n",
    "\n",
    "# ################\n",
    "# xgb_params = {\"objective\": \"reg:linear\", \"eta\": 0.01, \"max_depth\": 8, \"seed\": 42, \"silent\": 1}\n",
    "# num_rounds = 1000\n",
    "\n",
    "#dtrain = xgb.DMatrix(X_full, label=y_full)\n",
    "#gbdt = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "\n",
    "#importance = gbdt.get_fscore()#fmap='xgb.fmap')\n",
    "#importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#importance = pd.DataFrame(importance)\n",
    "\n",
    "#xgb.plot_importance(gbdt)\n",
    "#plt.show()\n",
    "\n",
    "# self.test_model = self.model\n",
    "#             self.test_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#             # make predictions for test data and evaluate\n",
    "#             y_pred = self.test_model.predict(X_test)\n",
    "#             if y.iloc[:,0].dtype != object:\n",
    "#                 predictions = [round(value) for value in y_pred]\n",
    "#             else:\n",
    "#                 predictions = y_pred\n",
    "#             accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "#             # Fit model using each importance as a threshold\n",
    "#             thresholds = sorted(self.test_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "None\n",
      "class\n",
      "argument\n",
      "None\n",
      "None\n",
      "argument\n",
      "None\n",
      "None\n",
      "True\n",
      "['__module__', 'apple', 'bottom', '__init__', 'varses', '__dict__', '__weakref__', '__doc__']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#         for v in range(0, len(self.variables)-1):\n",
    "#             if self.class_vars[v] is not None:\n",
    "#                 self.instance_variables[v] = self.class_vars[v]\n",
    "#             else:\n",
    "#                 self.instance_variables[v] = self.arguments[v]\n",
    "\n",
    "#         variables = ['booster', 'best_params', 'objective', 'random_state', 'test_size', 'scorer']\n",
    "#         for var in variables:\n",
    "#             instance_variable = eval(\"self.\"+var)\n",
    "#             class_variable = eval('Feature_Selector.'+var)\n",
    "#             variable = eval(var)\n",
    "#             if class_variable is not None:\n",
    "#                 instance_variable = class_variable\n",
    "#             else:\n",
    "#                 instance_variable = variable\n",
    "        \n",
    "#         self.booster=booster\n",
    "#         self.best_params=best_params\n",
    "#         self.objective=objective\n",
    "#         self.random_state=random_state\n",
    "#         self.test_size=test_size\n",
    "#         self.scorer=scorer\n",
    "\n",
    "\"\"\"HOW ON EARTH DOES ONE GET NAMES OF ARGUMENTS PASSED THROUGH __INIT___\"\"\"\n",
    "class dat:\n",
    "    apple = 'a'\n",
    "    bottom = 'b'\n",
    "    def __init__(self,a=1, b=2, c=3):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "        self.class_variables = list(vars(dat).keys())\n",
    "    def varses(self,run=1):\n",
    "        return self.class_variables\n",
    "\n",
    "# dat().varses(2)\n",
    "\"\"\"THIS ONE ONLY WORKS ON FUNCTIONS\"\"\"\n",
    "#dat.__code__.co_varnames\n",
    "\n",
    "AP = 1\n",
    "A = \"A\"\n",
    "P = \"P\"\n",
    "word = eval(A + P)\n",
    "print(word)\n",
    "\n",
    "#arguments\n",
    "a = 'argument'\n",
    "b = 'argument'\n",
    "\n",
    "#instance\n",
    "ia = None\n",
    "ib = None\n",
    "\n",
    "#class\n",
    "ca = 'class'\n",
    "cb = None\n",
    "\n",
    "variables = ['a', 'b']\n",
    "for var in variables:\n",
    "    instance_variable = eval(\"i\"+var)\n",
    "    print(instance_variable)\n",
    "    class_variable = eval('c'+var)\n",
    "    print(class_variable)\n",
    "    variable = eval(var)\n",
    "    print(variable)\n",
    "    if class_variable is not None:\n",
    "        instance_variable = class_variable\n",
    "    else:\n",
    "        instance_variable = variable\n",
    "print(ia)\n",
    "print(ib)\n",
    "da = dat()\n",
    "print(callable(getattr(dat(), 'varses')))\n",
    "# two ways to get lists of class variables\n",
    "members = [attr for attr in dir(da) if not callable(getattr(da, attr)) and not attr.startswith(\"__\")]\n",
    "members\n",
    "print(list(vars(dat).keys()))\n",
    "variables = list(dat().varses())\n",
    "# getting values from a list between two specified values\n",
    "variables = variables[len(variables) - variables[::-1].index('__module__') : variables.index('__init__')]\n",
    "variables\n",
    "\n",
    "# getting boolean values using eval()\n",
    "eval(A+\"==\"+P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail\n"
     ]
    }
   ],
   "source": [
    "AB = 1\n",
    "variables =['B']\n",
    "class_name = \"A\"\n",
    "for v in variables:\n",
    "    if eval(\"%s == None\" % (class_name+v)) is True:\n",
    "        print('Success')\n",
    "    else:\n",
    "        print('Fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Selection Class # #\n",
    "\n",
    "class Feature_Selector:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "    This feature selection transformer works only with the following boosters as base learners:\n",
    "        -gbtree\n",
    "        -dart\n",
    "                                                                            \n",
    "    Sklearn.metrics scoring functions can be passed to parameter:\n",
    "        scorer = 'scoring_function'\n",
    "    \n",
    "    Built-in XGBoost evaluation metrics will be used by passing:\n",
    "        scorer = 'Auto'                                                    \n",
    "    \n",
    "    This class requires that X (features or independent variables) has already been encoded and Imputed.\n",
    "    \n",
    "    Tuning requires the target (y) to also be passed.\n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    # Compatible model evaluation metrics: Sklearn metrics\n",
    "    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,\n",
    "        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,\n",
    "                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,\n",
    "                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,\n",
    "                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,\n",
    "                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}\n",
    "\n",
    "    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']\n",
    "    \n",
    "    # metrics with which higher value = higher model performance\n",
    "    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',\n",
    "                                  'precision_score','recall_score',\n",
    "                                'explained_variance_score','r2_score','Average test auc',\n",
    "                                'Average test ndcg','Average test map']\n",
    "    # metrics with which lower value = higher model performance\n",
    "    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',\n",
    "                           'mean_squared_error','mean_squared_log_error','median_absolute_error',\n",
    "                           'Average test error','Average test rmse','Average test mae','Average test log loss',\n",
    "                           'Average test merror','Average test mlogloss']\n",
    "    \n",
    "    booster=None\n",
    "    best_params=None\n",
    "    objective=None\n",
    "    random_state=None\n",
    "    test_size=None\n",
    "    scorer=None\n",
    "\n",
    "    def __init__(self, booster='gbtree', objective='reg:logistic', \n",
    "                 random_state=69, test_size=0.2, best_params={}, scorer='accuracy_score',\n",
    "                \n",
    "                 # values passed for these parameters will never be used unless: class.parameter is made None\n",
    "                 scoring_functions=None, classification_objectives=None, regression_objectives=None,\n",
    "                the_higher_the_better=None, the_lower_the_better=None):\n",
    "        \n",
    "        \n",
    "        # assigning parameters as instance variables\n",
    "        varses = list(vars(Feature_Selector).keys())\n",
    "        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "        class_name = \"Feature_Selector\"+\".\"\n",
    "        for v in self.variables:\n",
    "            # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "            if eval(\"%s != None\" % (class_name+v)) is True:\n",
    "                exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "            # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "            else:\n",
    "                exec(\"self.%s = %s\" % (v, v))\n",
    "        \n",
    "    \n",
    "        # overriding colsample parameters\n",
    "        self.best_params['colsample_bytree'] = 1\n",
    "        self.best_params['colsample_bylevel'] = 1\n",
    "        \n",
    "        # determining the model build used for feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            if self.objective in Feature_Selector.classification_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBClassifier(**self.best_params)\n",
    "            elif self.objective in Feature_Selector.regression_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        elif self.booster == 'dart':\n",
    "            self.best_params['booster'] = self.booster\n",
    "            self.best_params['objective'] = self.objective\n",
    "        \n",
    "        \n",
    "        # making sure there is a scorer\n",
    "        if self.scorer not in Feature_Selector.the_higher_the_better and scorer not in Feature_Selector.the_lower_the_better:\n",
    "            self.scorer = 'Auto'\n",
    "    \n",
    "    def fit(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = X.drop(self.target, axis =1)\n",
    "        \n",
    "        # Encode labels if string when needed\n",
    "        if self.booster == 'dart' and self.objective in Feature_Selector.classification_objectives or self.scorer == 'f1_score':\n",
    "            if y.iloc[:,0].dtype == object:\n",
    "                Target_is_string = True\n",
    "            else:\n",
    "                Target_is_string = False\n",
    "            \n",
    "            if Target_is_string == True:\n",
    "                L_E = LabelEncoder()\n",
    "                L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "                y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "\n",
    "        # fit model on all training data            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                            test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "        \n",
    "        # Initialize lists of building performance metrics table\n",
    "        Threshold = []\n",
    "        ns = []\n",
    "        Scores = []\n",
    "        \n",
    "        # tree boosters: feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            self.test_model = self.model\n",
    "            self.test_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "            # Fit model using each importance as a threshold\n",
    "            thresholds = sorted(self.test_model.feature_importances_)\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                # select features using threshold\n",
    "                selection = SelectFromModel(self.test_model, threshold=thresh, prefit=True)\n",
    "                select_X_train = selection.transform(X_train)\n",
    "                select_X_test = selection.transform(X_test)\n",
    "                # train model\n",
    "                if self.objective in Feature_Selector.classification_objectives:\n",
    "                    estimator = xgb.XGBClassifier\n",
    "                if self.objective in Feature_Selector.regression_objectives:\n",
    "                    estimator = xgb.Regressor\n",
    "                selection_model = estimator(**self.best_params)\n",
    "                selection_model.fit(select_X_train, y_train.values.ravel(),\n",
    "                                   eval_set=[(select_X_train,y_train.values.ravel()),\n",
    "                                             (select_X_test,y_test.values.ravel())], verbose=False)\n",
    "                \n",
    "                # eval model\n",
    "                # using build-in evaluation metrics automatically matched with objective\n",
    "                if self.scorer == 'Auto':\n",
    "                    result = selection_model.evals_result()\n",
    "                    scorer_used = list(result['validation_1'].keys())[0]\n",
    "                    score = np.mean(result['validation_1'][scorer_used])\n",
    "                    Scores.append(score)\n",
    "                    ns.append(len(pd.DataFrame(select_X_train).columns))\n",
    "                    Threshold.append(thresh)\n",
    "\n",
    "                # using Sklearn metrics\n",
    "                else:\n",
    "                    scoring_function = Feature_Selector.scoring_functions[self.scorer]\n",
    "                    select_X_test = selection.transform(X_test)\n",
    "                    y_pred = selection_model.predict(select_X_test)\n",
    "                    if y_pred.dtype != object:\n",
    "                        predictions = [round(value) for value in y_pred]\n",
    "                        y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                    else:\n",
    "                        predictions = y_pred\n",
    "                        y_test_used = y_test\n",
    "                    score = scoring_function(y_test_used, predictions)\n",
    "                    if self.scorer == 'accuracy_score':\n",
    "                        score = score*100.00\n",
    "                    Scores.append(score)\n",
    "                    Threshold.append(thresh)\n",
    "                    ns.append(select_X_train.shape[1])\n",
    "        \n",
    "        # dart boosters: feature selection\n",
    "        elif self.booster == 'dart':\n",
    "            dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "            \n",
    "            # train the dart booster model\n",
    "            xg_reg = xgb.train(dtrain=dtrain, params=self.best_params, num_boost_round=10)\n",
    "            importance = xg_reg.get_fscore()\n",
    "            importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            importance = pd.DataFrame(importance)\n",
    "        \n",
    "            for value in range(1, len(importance)):\n",
    "                columns = importance.iloc[:,0][:value]\n",
    "                dtrain_trim = xgb.DMatrix(data=X_train.loc[:,columns], label=y_train)\n",
    "                dtest_trim = xgb.DMatrix(data=X_test.loc[:,columns], label=y_test)\n",
    "                \n",
    "                # eval model\n",
    "                # using build-in evaluation metrics automatically matched with objective\n",
    "                if self.scorer == 'Auto':\n",
    "                    result = {}\n",
    "                    selection_model = xgb.train(dtrain=dtrain_trim, params=self.best_params,\n",
    "                                                evals=[(dtest_trim, 'eval')], evals_result=result,\n",
    "                                               verbose_eval=False)\n",
    "                    scorer_used = list(result['eval'].keys())[0]\n",
    "                    score = np.mean(result['eval'][scorer_used])\n",
    "                    Scores.append(score)\n",
    "                    ns.append(len(columns))\n",
    "                    Threshold.append(importance.iloc[value-1,1])\n",
    "                \n",
    "                # using Sklearn metrics\n",
    "                else:\n",
    "                    scoring_function = Feature_Selector.scoring_functions[self.scorer]\n",
    "\n",
    "                    selection_model = xgb.train(dtrain=dtrain_trim, params=self.best_params)\n",
    "                    y_pred = selection_model.predict(dtest_trim)\n",
    "                    if y_pred.dtype != object:\n",
    "                        predictions = [round(value) for value in y_pred]\n",
    "                        y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                    else:\n",
    "                        predictions = y_pred\n",
    "                        y_test_used\n",
    "                    score = scoring_function(y_test_used, predictions)\n",
    "                    if self.scorer == 'accuracy_score':\n",
    "                        score = score*100.00\n",
    "                    Scores.append(score)\n",
    "                    ns.append(len(columns))\n",
    "                    Threshold.append(importance.iloc[value-1,1])\n",
    "        \n",
    "        # building table of performance scores\n",
    "        if self.scorer == 'Auto':\n",
    "            self.scorer = 'Average test ' + scorer_used\n",
    "        self.performance_scores = pd.DataFrame()\n",
    "        self.performance_scores['Threshold'] = Threshold\n",
    "        self.performance_scores['n'] = ns\n",
    "        self.performance_scores[self.scorer] = Scores\n",
    "\n",
    "        # Best cut-off of top features: minumum number giving the max possible model performance \n",
    "        if self.scorer in Feature_Selector.the_higher_the_better:\n",
    "            self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                     == max(self.performance_scores[self.scorer])]\n",
    "            self.best_threshold = self.best_perf[(self.best_perf['n'] == min(self.best_perf['n']))].drop_duplicates()\n",
    "        elif self.scorer in Feature_Selector.the_lower_the_better:\n",
    "            self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                     == min(self.performance_scores[self.scorer])]\n",
    "            self.best_threshold = self.best_perf[(self.best_perf['n'] == min(self.best_perf['n']))].drop_duplicates()\n",
    "        \n",
    "        # store the best number of features\n",
    "        self.best_n = self.best_threshold.n.iloc[0]\n",
    "        \n",
    "        # saving selected columns for use on .predict()\n",
    "        if self.booster == 'gbtree':\n",
    "            self.selected_columns = X.columns[np.argsort(self.test_model.feature_importances_)\\\n",
    "                                          [-(self.best_n):]]\n",
    "        if self.booster == 'dart':\n",
    "            self.selected_columns = list(columns[:self.best_n])   \n",
    "        \n",
    "        # reverse label encoding\n",
    "        if self.booster == 'dart' and self.objective in Feature_Selector.classification_objectives:\n",
    "            if Target_is_string == True:\n",
    "                y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        # Drop unwanted features\n",
    "        X = X[self.selected_columns]\n",
    "        \n",
    "        return pd.DataFrame(X), pd.DataFrame(y)\n",
    "    \n",
    "    def fit_transform(self, X, y=None, target=None):\n",
    "        self.fit(X, y, target)\n",
    "        return self.transform(X, y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree\n",
      "dart\n"
     ]
    }
   ],
   "source": [
    "Feature_Hyperscaler.booster = 'gbtree'\n",
    "FS1 = Feature_Hyperscaler(booster='dart')\n",
    "print(FS1.booster)\n",
    "Feature_Hyperscaler.booster = 'dart'\n",
    "FS2 = Feature_Hyperscaler(booster='gblinear')\n",
    "print(FS2.booster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gbtree'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#             # make predictions for test data and evaluate\n",
    "#             y_pred = self.test_model.predict(X_test)\n",
    "#             if y.iloc[:,0].dtype != object:\n",
    "#                 predictions = [round(value) for value in y_pred]\n",
    "#             else:\n",
    "#                 predictions = y_pred\n",
    "#             accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "ffs = Feature_Selector()\n",
    "ffs.booster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives: guide\n",
    "https://github.com/dmlc/xgboost/blob/master/doc/parameter.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Binary_Undersampler:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "    This class is applicable only on data for binary classification.\n",
    "    This function assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "\n",
    "        # Encoding target variables formatted as string\n",
    "        if y.iloc[:,0].dtype == object:\n",
    "            Target_is_string = True\n",
    "        else:\n",
    "            Target_is_string = False\n",
    "\n",
    "        if Target_is_string == True:\n",
    "            L_E = LabelEncoder()\n",
    "            L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "            y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "    \n",
    "        Positive_count = y.values.sum()\n",
    "        Positive_indices = np.array(y[y.iloc[:,-1] ==1].index)\n",
    "        Negative_indices = y[y.iloc[:,-1] == 0].index\n",
    "        #return X, y\n",
    "        random_Negative_indices = np.array(np.random.choice(Negative_indices, Positive_count, replace = False))\n",
    "        Undersample_indices = np.concatenate([Positive_indices, random_Negative_indices])\n",
    "        \n",
    "        # Restore original labels\n",
    "        if Target_is_string == True:\n",
    "            y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "                \n",
    "        return X.loc[Undersample_indices,:].reset_index(drop=True), y.loc[Undersample_indices,:].reset_index(drop=True)\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for transforming to dictionary\n",
    "\n",
    "class Dictionator:\n",
    "    def __init__(self, orient=\"records\"):\n",
    "        self.orient = orient\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "# Class for transforming to DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionator:\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "    \n",
    "    def __init__(self, orient=\"records\", objective=\"reg:linear\"):\n",
    "        self.orient = orient\n",
    "        self.objective = objective\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        if self.objective in classification_objectives:\n",
    "            return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        return X.to_dict(self.orient), y\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy Variable-creator class\n",
    "class Dummynatrix:\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "        \n",
    "    def __init__(self, sparse=False, objective=\"reg:linear\"):\n",
    "        self.sparse = sparse\n",
    "        self.objective = objective\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y):\n",
    "        if self.objective in Dummynatrix.classification_objectives:\n",
    "            return DictVectorizer(sparse=self.sparse).fit_transform(X), \\\n",
    "                        DictVectorizer(sparse=self.sparse).fit_transform(y)\n",
    "        return DictVectorizer(sparse=self.sparse).fit_transform(X), y\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for transforming to DMatrix\n",
    "class DMatrix_Reloader:\n",
    "    \n",
    "    def fit(self, data, label):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, label):\n",
    "        return xgb.DMatrix(data= data, label= label )\n",
    "    \n",
    "    def fit_transform(self, data, label):\n",
    "        self.fit(data, label)\n",
    "        return self.transform(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling Class\n",
    "\n",
    "class HyperSampler:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for resampling methods methods (parameter sampler='') are the following:\n",
    "            Binary_Undersampler, ClusterCentroids, CondensedNearestNeighbour, EditedNearestNeighbours,\n",
    "            RepeatedEditedNearestNeighbours, AllKNN, InstanceHardnessThreshold, NearMiss, NeighbourhoodCleaningRule, \n",
    "            OneSidedSelection, RandomUnderSampler, TomekLinks, ADASYN, RandomOverSampler, SMOTE, SMOTEENN, \n",
    "            SMOTETomek, BalanceCascade, BalancedBaggingClassifier, EasyEnsemble\n",
    ".\n",
    "        \n",
    "        If sampler = 'HyperSampler',\n",
    "            every resampling method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The resampling method that contributes to the best score will be applied to the data to be returned.\n",
    "            \n",
    "            HyperSampler.resampler = chosen resampling class assigned after instantiation \n",
    "                                                                        or after fit() if sampler = 'HyperSampler'\n",
    "        \n",
    "        This feature selection transformer works only with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        Tuning requires the target (y) to also be passed.\n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    # Compatible model evaluation metrics: Sklearn metrics\n",
    "    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,\n",
    "        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,\n",
    "                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,\n",
    "                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,\n",
    "                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,\n",
    "                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}\n",
    "\n",
    "    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']\n",
    "    \n",
    "    # metrics with which higher value = higher model performance\n",
    "    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',\n",
    "                                  'precision_score','recall_score',\n",
    "                                'explained_variance_score','r2_score','Average test auc',\n",
    "                                'Average test ndcg','Average test map']\n",
    "    # metrics with which lower value = higher model performance\n",
    "    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',\n",
    "                           'mean_squared_error','mean_squared_log_error','median_absolute_error',\n",
    "                           'Average test error','Average test rmse','Average test mae','Average test log loss',\n",
    "                           'Average test merror','Average test mlogloss']\n",
    "\n",
    "    # parameters to inherited from mother class\n",
    "    sampler=None\n",
    "    best_params=None\n",
    "    booster=None\n",
    "    objective=None\n",
    "    scorer=None\n",
    "    \n",
    "    estimator=None\n",
    "    voting=None\n",
    "    n_jobs=None\n",
    "    return_indices=None\n",
    "    size_ngh=None\n",
    "    n_neighbors=None\n",
    "    n_seeds_S=None\n",
    "    n_neighbors=None\n",
    "    kind_sel=None\n",
    "    max_iter=None\n",
    "    allow_minority=None\n",
    "    ratio=None\n",
    "    cv=None\n",
    "    version=None\n",
    "    ver3_samp_ngh=None\n",
    "    n_neighbors_ver3=None\n",
    "    threshold_cleaning=None\n",
    "    replacement=None\n",
    "    k=None\n",
    "    n_neighbors=None\n",
    "    k_neighbors=None\n",
    "    m=None\n",
    "    m_neighbors=None\n",
    "    out_step=None\n",
    "    kind=None\n",
    "    svm_estimator=None\n",
    "    smote=None\n",
    "    enn=None\n",
    "    out_step=None\n",
    "    kind_smote=None\n",
    "    kind_enn=None\n",
    "    n_jobs=None\n",
    "    tomek=None\n",
    "    n_max_subset=None\n",
    "    classifier=None\n",
    "    n_estimators=None\n",
    "    max_samples=None\n",
    "    max_features=None\n",
    "    bootstrap=None\n",
    "    bootstrap_features=None\n",
    "    oob_score=None\n",
    "    warm_start=None\n",
    "    verbose=None\n",
    "    n_subsets=None\n",
    "\n",
    "#     **kwargs\n",
    "    \n",
    "    random_state=None\n",
    "    test_size=None\n",
    "    \n",
    "    def __init__(self, scale=\"HyperSampler\", best_params={},\n",
    "                 booster='gbtree', objective='reg:logistic', scorer='accuracy_score',\n",
    "                 # parameters for samplers\n",
    "                 estimator=None,voting='auto',n_jobs=1, return_indices=False,size_ngh=None,n_neighbors=None,n_seeds_S=1,\n",
    "                 n_neighbors=3,kind_sel='all',max_iter=100,allow_minority=False,ratio='auto',cv=5,version=1,\n",
    "                 ver3_samp_ngh=None,n_neighbors_ver3=3,threshold_cleaning=0.5,replacement=False,k=None,n_neighbors=5,\n",
    "                 k_neighbors=5,m=None,m_neighbors=10,out_step=0.5,kind='regular',svm_estimator=None,smote=None,\n",
    "                 enn=None,out_step=None,kind_smote=None,kind_enn=None,n_jobs=None,tomek=None,n_max_subset=None,\n",
    "                 classifier=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,\n",
    "                 oob_score=False,warm_start=False,verbose=0,n_subsets=10,\n",
    "                # parameters for train/test split\n",
    "                 random_state=69, test_size=0.2,\n",
    "                # values passed for these parameters will never be used unless: class.parameter is made None\n",
    "                scoring_functions=None, classification_objectives=None,\n",
    "                 the_higher_the_better=None, the_lower_the_better=None):\n",
    "        \n",
    "        # assigning parameters as instance variables\n",
    "        varses = list(vars(HyperSampler).keys())\n",
    "        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "        class_name = \"HyperSampler\"+\".\"\n",
    "        for v in self.variables:\n",
    "            # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "            if eval(\"%s != None\" % (class_name+v)) is True:\n",
    "                exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "            # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "            else:\n",
    "                exec(\"self.%s = %s\" % (v, v))\n",
    "        \n",
    "        # determining the model build used for feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            if self.objective in HyperSampler.classification_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBClassifier(**self.best_params)\n",
    "\n",
    "            elif self.objective in HyperSampler.regression_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "            self.best_params['booster'] = self.booster\n",
    "            self.best_params['objective'] = self.objective\n",
    "        \n",
    "        self.sampler_params={'StandardScaler':[StandardScaler,{'copy':self.copy, \n",
    "                                              'with_mean':self.with_mean, 'with_std':self.with_std}],\n",
    "                          'MinMaxScaler':[MinMaxScaler,{'feature_range':self.feature_range, 'copy':self.copy}],\n",
    "                          'Normalizer':[Normalizer,{'norm':self.norm, 'copy':self.copy}],\n",
    "                            'RobustScaler':[RobustScaler,{'with_centering':self.with_centering,\n",
    "                                            'with_scaling':self.with_scaling,\n",
    "                                             'quantile_range':self.quantile_range, 'copy':self.copy}],\n",
    "                           'MaxAbsScaler':[MaxAbsScaler,{'copy':self.copy}],\n",
    "                           'QuantileTransformer':[QuantileTransformer,{'n_quantiles':self.n_quantiles, \n",
    "                                                  'output_distribution':self.output_distribution,\n",
    "                                                  'ignore_implicit_zeros':self.ignore_implicit_zeros,\n",
    "                                                  'subsample':self.subsample,'random_state':self.random_state}]}\n",
    "        \n",
    "        # Assigning the resampler if sampler (sampling method) is not \"Hyperscale\"\n",
    "        if self.sampler is not 'HyperSampler':\n",
    "            self.resampler = self.sampler_params[self.sampler][0](**self.sampler_params[self.sampler][1])\n",
    "    \n",
    "    def fit(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if self.target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[self.target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        \n",
    "        if self.sampler == 'HyperSampler':\n",
    "            # Encode labels if string when needed\n",
    "            if self.booster == 'dart' and self.objective in HyperSampler.classification_objectives\\\n",
    "                                                                    or self.scorer == 'f1_score':\n",
    "                if y.iloc[:,0].dtype == object:\n",
    "                    Target_is_string = True\n",
    "                else:\n",
    "                    Target_is_string = False\n",
    "\n",
    "                if Target_is_string == True:\n",
    "                    L_E = LabelEncoder()\n",
    "                    L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "                    y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "\n",
    "            # fit model on all training data            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "            \n",
    "            samplers = ['None','Binary_Undersampler', 'ClusterCentroids', 'CondensedNearestNeighbour', 'EditedNearestNeighbours',\n",
    "            'RepeatedEditedNearestNeighbours', 'AllKNN', 'InstanceHardnessThreshold', 'NearMiss', 'NeighbourhoodCleaningRule', \n",
    "            'OneSidedSelection', 'RandomUnderSampler', 'TomekLinks', 'ADASYN', 'RandomOverSampler', 'SMOTE', 'SMOTEENN', \n",
    "            'SMOTETomek', 'BalanceCascade', 'BalancedBaggingClassifier', 'EasyEnsemble']\n",
    "            \n",
    "            Scores = []\n",
    "            for sampler in samplers:\n",
    "                if sampler is 'None':\n",
    "                    X_train_resampled, y_train_resampled = X_train, y_train\n",
    "                    X_test_resampled, y_test_resampled = X_test, y_test\n",
    "                else:\n",
    "                    sampler = self.sampler_params[sampler][0](**self.sampler_params[sampler][1])\n",
    "                    X_train_resampled, y_train_resampled = sampler.fit_sample(X_train, y_train)\n",
    "                    X_test_resampled, y_test_resampled = sampler.fit_sample(X_test, y_test)\n",
    "                \n",
    "                if self.booster is 'gbtree':\n",
    "                    if self.objective in HyperSampler.classification_objectives:\n",
    "                        estimator = xgb.XGBClassifier\n",
    "                    if self.objective in HyperSampler.regression_objectives:\n",
    "                        estimator = xgb.Regressor\n",
    "                    selection_model = estimator(**self.best_params)\n",
    "                    selection_model.fit(X_train_scaled, y_train.values.ravel(),\n",
    "                                       eval_set=[(X_train_resampled,y_train_resampled.values.ravel()),\n",
    "                                                 (X_test_resampled,y_test_resampled.values.ravel())], verbose=False)\n",
    "                    # eval model\n",
    "                    # using build-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = selection_model.evals_result()\n",
    "                        scorer_used = list(result['validation_1'].keys())[0]\n",
    "                        score = np.mean(result['validation_1'][scorer_used])\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = HyperSampler.scoring_functions[self.scorer]\n",
    "                        y_pred = selection_model.predict(X_test_scaled)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "                        \n",
    "                elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "                    dtrain_resampled = xgb.DMatrix(data=X_train_resampled, label=y_train_resampled)\n",
    "                    dtest_resampled = xgb.DMatrix(data=X_test_resampled, label=y_test_resampled)\n",
    "\n",
    "                    # eval model\n",
    "                    # using build-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = {}\n",
    "                        selection_model = xgb.train(dtrain=dtrain_resampled, params=self.best_params,\n",
    "                                                    evals=[(dtest_resampled, 'eval')], evals_result=result,\n",
    "                                                   verbose_eval=False)\n",
    "                        scorer_used = list(result['eval'].keys())[0]\n",
    "                        score = np.mean(result['eval'][scorer_used])\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = HyperSampler.scoring_functions[self.scorer]\n",
    "\n",
    "                        selection_model = xgb.train(dtrain=dtrain_resampled, params=self.best_params)\n",
    "                        y_pred = selection_model.predict(dtest_resampled)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "\n",
    "            # building table of performance scores\n",
    "            if self.scorer == 'Auto':\n",
    "                self.scorer = 'Average test ' + scorer_used\n",
    "            self.performance_scores = pd.DataFrame()\n",
    "            self.performance_scores['Samplers'] = samplers\n",
    "            self.performance_scores[self.scorer] = Scores\n",
    "\n",
    "            # Best scaler: returns max possible model performance \n",
    "            if self.scorer in HyperSampler.the_higher_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == max(self.performance_scores[self.scorer])]\n",
    "            elif self.scorer in HyperSampler.the_lower_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == min(self.performance_scores[self.scorer])]\n",
    "                \n",
    "            # assigning the best-performing scaler as scaler\n",
    "            self.best_sampler = self.best_perf.iloc[0,0]\n",
    "            if self.best_sampler is not 'None':\n",
    "                self.resampler = self.sampler_params[self.best_sampler][0](**self.sampler_params[self.best_sampler][1])\n",
    "            else:\n",
    "                self.resampler = None\n",
    "            \n",
    "            # reverse label encoding\n",
    "            if self.booster == 'dart' and self.objective in HyperSampler.classification_objectives:\n",
    "                if Target_is_string == True:\n",
    "                    y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        else:\n",
    "            return self\n",
    "    \n",
    "    def sample(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        if self.best_sampler is not 'None':\n",
    "            X_resampled, y_resampled = self.resampler.fit_sample(X, y)\n",
    "            return pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=y.columns)\n",
    "        else:\n",
    "            return pd.DataFrame(X), pd.DataFrame(y)\n",
    "        \n",
    "    def fit_sample(self, X, y=None, target=None):\n",
    "        self.fit(X,y, target=target)\n",
    "        return self.sample(X, y, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This class handles the imputation of missing values\n",
      "        and the encoding and creation of dummy variables for categorical variables.\n",
      "\n",
      "        If the target variable is passed as y, it will also be imputed if impute_y= True.\n",
      "        Otherwise, y will pass through as untouched.\n"
     ]
    }
   ],
   "source": [
    "print(Imputation_Nation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom = pd.read_csv('mushrooms.csv')#.dropna()\n",
    "# df = Mushroom\n",
    "# X = Mushroom.drop('class',axis=1)\n",
    "# y = pd.DataFrame(Mushroom['class'])\n",
    "# print(type(X))\n",
    "#print(y)\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df = df.drop('Time', axis=1)\n",
    "df.reset_index(inplace=True)\n",
    "target_name = 'Class'\n",
    "X = df.drop(target_name, axis =1)\n",
    "y = pd.DataFrame(df[target_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2      2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3      3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4      4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "5      5 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "6      6  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7      7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8      8 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "9      9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "\n",
       "         V7        V8        V9   ...         V20       V21       V22  \\\n",
       "0  0.239599  0.098698  0.363787   ...    0.251412 -0.018307  0.277838   \n",
       "1 -0.078803  0.085102 -0.255425   ...   -0.069083 -0.225775 -0.638672   \n",
       "2  0.791461  0.247676 -1.514654   ...    0.524980  0.247998  0.771679   \n",
       "3  0.237609  0.377436 -1.387024   ...   -0.208038 -0.108300  0.005274   \n",
       "4  0.592941 -0.270533  0.817739   ...    0.408542 -0.009431  0.798278   \n",
       "5  0.476201  0.260314 -0.568671   ...    0.084968 -0.208254 -0.559825   \n",
       "6 -0.005159  0.081213  0.464960   ...   -0.219633 -0.167716 -0.270710   \n",
       "7  1.120631 -3.807864  0.615375   ...   -0.156742  1.943465 -1.015455   \n",
       "8  0.370145  0.851084 -0.392048   ...    0.052736 -0.073425 -0.268092   \n",
       "9  0.651583  0.069539 -0.736727   ...    0.203711 -0.246914 -0.633753   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Amount  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "5 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67  \n",
       "6 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99  \n",
       "7  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.80  \n",
       "8 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20  \n",
       "9 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y.iloc[:,0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_nulled = Nullifier().df_null_dictionary(Mushroom, {'class':['e']})\n",
    "# y_nulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nonull, y_nonull = MCAR_Dropper().fit_transform(X, y)\n",
    "y_nonull.iloc[:,0].unique()\n",
    "type(y_nonull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_imputed = Imputation_Nation(objective='reg:logistic').fit_transform(X_nonull)\n",
    "y_imputed = Imputation_Nation().fit_transform(y_nonull)\n",
    "#X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_imputed, y_imputed = Imputation_Nation(objective=\"reg:logistic\", impute_y=True).fit_transform(X, y)\n",
    "y_imputed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984\n",
      "984\n",
      "Counter({1: 492, 0: 492})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "X_undersampled, y_undersampled = Binary_Undersampler().fit_transform(X_imputed, y_imputed)\n",
    "print(len(X_undersampled))\n",
    "print(len(y_undersampled))\n",
    "print(Counter(y_undersampled.iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 492, 1: 492})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_undersampled.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scalers</th>\n",
       "      <th>Average test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.043452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>0.070863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>0.104772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normalizer</td>\n",
       "      <td>0.060508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>0.064061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>0.056244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QuantileTransformer</td>\n",
       "      <td>0.066345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Scalers  Average test error\n",
       "0                 None            0.043452\n",
       "1       StandardScaler            0.070863\n",
       "2         MinMaxScaler            0.104772\n",
       "3           Normalizer            0.060508\n",
       "4         RobustScaler            0.064061\n",
       "5         MaxAbsScaler            0.056244\n",
       "6  QuantileTransformer            0.066345"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales = ['None','StandardScaler','MinMaxScaler','Normalizer',\n",
    "          'RobustScaler','MaxAbsScaler','QuantileTransformer']\n",
    "FSS = Feature_Hyperscaler(booster='dart',objective='binary:logistic', scorer='Auto')\n",
    "scale = 'MaxAbsScaler'\n",
    "\n",
    "FSS.fit(X_undersampled, y_undersampled)\n",
    "X_scaled, y_scaled = FSS.transform(X_undersampled, y_undersampled)\n",
    "y_scaled.iloc[:,0].unique()\n",
    "\n",
    "pd.DataFrame(X_scaled).head(10)\n",
    "FSS.performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scalers</th>\n",
       "      <th>Average test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.043452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scalers  Average test error\n",
       "0    None            0.043452"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FSS.best_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSS.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class\n",
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Amount': 11,\n",
       " 'V1': 20,\n",
       " 'V10': 12,\n",
       " 'V11': 6,\n",
       " 'V12': 19,\n",
       " 'V13': 2,\n",
       " 'V14': 29,\n",
       " 'V15': 3,\n",
       " 'V16': 5,\n",
       " 'V17': 12,\n",
       " 'V18': 3,\n",
       " 'V19': 11,\n",
       " 'V2': 11,\n",
       " 'V20': 6,\n",
       " 'V21': 8,\n",
       " 'V22': 8,\n",
       " 'V23': 6,\n",
       " 'V24': 4,\n",
       " 'V25': 4,\n",
       " 'V26': 3,\n",
       " 'V27': 3,\n",
       " 'V28': 3,\n",
       " 'V3': 15,\n",
       " 'V4': 40,\n",
       " 'V5': 7,\n",
       " 'V6': 3,\n",
       " 'V7': 13,\n",
       " 'V8': 8,\n",
       " 'V9': 4,\n",
       " 'index': 35}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms = {'booster':'dart','objective':'reg:linear'}\n",
    "dtrain = xgb.DMatrix(data=X_scaled, label=y_scaled)\n",
    "linear = xgb.train(dtrain=dtrain, params=parms)\n",
    "linear.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V14</th>\n",
       "      <th>V4</th>\n",
       "      <th>V8</th>\n",
       "      <th>V19</th>\n",
       "      <th>V12</th>\n",
       "      <th>V16</th>\n",
       "      <th>V20</th>\n",
       "      <th>V23</th>\n",
       "      <th>V17</th>\n",
       "      <th>V10</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.289254</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>0.416956</td>\n",
       "      <td>-2.899907</td>\n",
       "      <td>-1.140747</td>\n",
       "      <td>0.126911</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>-2.830056</td>\n",
       "      <td>-2.772272</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.692029</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>0.283345</td>\n",
       "      <td>-0.503141</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>2.102339</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>0.599717</td>\n",
       "      <td>-0.838587</td>\n",
       "      <td>529.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.470102</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-1.334441</td>\n",
       "      <td>-6.560124</td>\n",
       "      <td>-2.282194</td>\n",
       "      <td>-0.430022</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-4.781831</td>\n",
       "      <td>-1.525412</td>\n",
       "      <td>239.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.771097</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>0.308334</td>\n",
       "      <td>-10.912819</td>\n",
       "      <td>-7.358083</td>\n",
       "      <td>-0.171608</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-12.598419</td>\n",
       "      <td>-4.801637</td>\n",
       "      <td>59.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.079337</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-2.721853</td>\n",
       "      <td>-4.609628</td>\n",
       "      <td>2.581851</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>6.739384</td>\n",
       "      <td>-2.447469</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-10.691196</td>\n",
       "      <td>6.675732</td>\n",
       "      <td>0.154612</td>\n",
       "      <td>-1.934666</td>\n",
       "      <td>-9.854485</td>\n",
       "      <td>-2.041974</td>\n",
       "      <td>0.488378</td>\n",
       "      <td>-0.539528</td>\n",
       "      <td>-1.129056</td>\n",
       "      <td>-6.187891</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-10.733854</td>\n",
       "      <td>6.348557</td>\n",
       "      <td>0.303253</td>\n",
       "      <td>-1.327357</td>\n",
       "      <td>-8.948179</td>\n",
       "      <td>-1.638960</td>\n",
       "      <td>0.587743</td>\n",
       "      <td>-0.669605</td>\n",
       "      <td>-1.746350</td>\n",
       "      <td>-6.045468</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9.177166</td>\n",
       "      <td>6.078266</td>\n",
       "      <td>0.133080</td>\n",
       "      <td>-2.370599</td>\n",
       "      <td>-8.873748</td>\n",
       "      <td>-0.871688</td>\n",
       "      <td>0.269773</td>\n",
       "      <td>-0.551572</td>\n",
       "      <td>1.313014</td>\n",
       "      <td>-5.134454</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-9.252307</td>\n",
       "      <td>6.047445</td>\n",
       "      <td>0.055586</td>\n",
       "      <td>-1.808012</td>\n",
       "      <td>-7.520117</td>\n",
       "      <td>-0.502362</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>-0.583813</td>\n",
       "      <td>0.784427</td>\n",
       "      <td>-4.959493</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-6.210258</td>\n",
       "      <td>4.007683</td>\n",
       "      <td>1.063728</td>\n",
       "      <td>2.250123</td>\n",
       "      <td>-7.148243</td>\n",
       "      <td>-3.599540</td>\n",
       "      <td>0.504646</td>\n",
       "      <td>0.601045</td>\n",
       "      <td>-4.830324</td>\n",
       "      <td>-4.624985</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V14        V4        V8       V19        V12       V16       V20  \\\n",
       "0  -4.289254  3.997906  1.391657  0.416956  -2.899907 -1.140747  0.126911   \n",
       "1  -1.692029  2.288644 -0.067794  0.283345  -0.503141  0.666780  2.102339   \n",
       "2  -1.470102  2.330243 -0.399147 -1.334441  -6.560124 -2.282194 -0.430022   \n",
       "3  -6.771097  2.679787 -0.248778  0.308334 -10.912819 -7.358083 -0.171608   \n",
       "4  -6.079337  4.732795 -0.496358 -2.721853  -4.609628  2.581851  0.009061   \n",
       "5 -10.691196  6.675732  0.154612 -1.934666  -9.854485 -2.041974  0.488378   \n",
       "6 -10.733854  6.348557  0.303253 -1.327357  -8.948179 -1.638960  0.587743   \n",
       "7  -9.177166  6.078266  0.133080 -2.370599  -8.873748 -0.871688  0.269773   \n",
       "8  -9.252307  6.047445  0.055586 -1.808012  -7.520117 -0.502362  0.388307   \n",
       "9  -6.210258  4.007683  1.063728  2.250123  -7.148243 -3.599540  0.504646   \n",
       "\n",
       "        V23        V17       V10  Amount  \n",
       "0 -0.465211  -2.830056 -2.772272    0.00  \n",
       "1  1.375966   0.599717 -0.838587  529.00  \n",
       "2  0.172726  -4.781831 -1.525412  239.93  \n",
       "3 -0.436207 -12.598419 -4.801637   59.00  \n",
       "4 -0.656805   6.739384 -2.447469    1.00  \n",
       "5 -0.539528  -1.129056 -6.187891    1.00  \n",
       "6 -0.669605  -1.746350 -6.045468    1.00  \n",
       "7 -0.551572   1.313014 -5.134454    1.00  \n",
       "8 -0.583813   0.784427 -4.959493    1.00  \n",
       "9  0.601045  -4.830324 -4.624985    1.00  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_scaled.iloc[:,0] = pd.get_dummies(y_scaled.iloc[:,0], drop_first=True, columns=[y_scaled.columns[0]])kwargs = {\"colsample_bytree\":0.8,\n",
    "       #\"gamma\":0.1, \"learning_rate\":0.2, \"colsample_bylevel\":0.9}\n",
    "#model = Feature_Selector(objective='count:poisson', best_params=kwargs).fit(X_scaled, y_scaled).test_model\n",
    "#print(model)\n",
    "#print(xgb.XGBClassifier())\n",
    "\n",
    "feat_selector = Feature_Selector(booster='dart',objective='binary:logistic', scorer='Auto')\n",
    "feat_selector = feat_selector.fit(X_scaled, y_scaled)\n",
    "\n",
    "\n",
    "X_processed, y_processed = feat_selector.transform(X_scaled, y_scaled)\n",
    "#print(feat_selector.test_model.feature_importances_)\n",
    "# print(X_scaled.columns[np.argsort(feat_selector.test_model.feature_importances_)[-(feat_selector.best_threshold.n.iloc[0]):]])\n",
    "X_processed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>n</th>\n",
       "      <th>Average test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.059899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.058884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.060914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.064467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.053807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.067005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.062436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.069543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.059899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.051776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.054315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.057868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.055837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.054315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.059391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.061929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>0.060914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0.059899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.062437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0.063959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0.067005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0.067005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0.064467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0.064975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0.069543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0.069543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.070051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold   n  Average test error\n",
       "0          20   1            0.084264\n",
       "1          18   2            0.059899\n",
       "2          11   3            0.058884\n",
       "3          10   4            0.060914\n",
       "4           9   5            0.064467\n",
       "5           8   6            0.053807\n",
       "6           7   7            0.067005\n",
       "7           7   8            0.062436\n",
       "8           7   9            0.069543\n",
       "9           6  10            0.059899\n",
       "10          6  11            0.051776\n",
       "11          5  12            0.054315\n",
       "12          5  13            0.057868\n",
       "13          5  14            0.055837\n",
       "14          5  15            0.054315\n",
       "15          4  16            0.059391\n",
       "16          4  17            0.061929\n",
       "17          4  18            0.060914\n",
       "18          4  19            0.059899\n",
       "19          4  20            0.062437\n",
       "20          3  21            0.063959\n",
       "21          3  22            0.067005\n",
       "22          3  23            0.067005\n",
       "23          3  24            0.064467\n",
       "24          3  25            0.064975\n",
       "25          2  26            0.069543\n",
       "26          2  27            0.069543\n",
       "27          1  28            0.070051"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_selector.performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>n</th>\n",
       "      <th>Average test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.051776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold   n  Average test error\n",
       "10          6  11            0.051776"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_selector.best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class\n",
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({numpy.int64: 11})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_done= PCA().fit_transform(X_processed)\n",
    "Counter([type(value) for value in pd.DataFrame(X_done).columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-98.856112</td>\n",
       "      <td>0.278069</td>\n",
       "      <td>-1.088056</td>\n",
       "      <td>-1.487752</td>\n",
       "      <td>0.771298</td>\n",
       "      <td>1.008360</td>\n",
       "      <td>-0.686408</td>\n",
       "      <td>0.491947</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.093287</td>\n",
       "      <td>-0.522633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430.136945</td>\n",
       "      <td>-6.091276</td>\n",
       "      <td>0.415409</td>\n",
       "      <td>-1.705227</td>\n",
       "      <td>1.505790</td>\n",
       "      <td>0.512664</td>\n",
       "      <td>-1.236220</td>\n",
       "      <td>-1.386649</td>\n",
       "      <td>-1.130202</td>\n",
       "      <td>1.139248</td>\n",
       "      <td>-0.233847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141.074143</td>\n",
       "      <td>0.689728</td>\n",
       "      <td>0.739069</td>\n",
       "      <td>1.181097</td>\n",
       "      <td>-0.449276</td>\n",
       "      <td>1.799765</td>\n",
       "      <td>3.680595</td>\n",
       "      <td>0.337945</td>\n",
       "      <td>-1.897309</td>\n",
       "      <td>-0.199442</td>\n",
       "      <td>0.201884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-39.843731</td>\n",
       "      <td>12.333006</td>\n",
       "      <td>1.114797</td>\n",
       "      <td>3.029670</td>\n",
       "      <td>-3.653070</td>\n",
       "      <td>0.582510</td>\n",
       "      <td>3.440398</td>\n",
       "      <td>0.487783</td>\n",
       "      <td>-1.248633</td>\n",
       "      <td>0.690207</td>\n",
       "      <td>0.589026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-97.871374</td>\n",
       "      <td>-5.085720</td>\n",
       "      <td>1.098542</td>\n",
       "      <td>-10.053107</td>\n",
       "      <td>3.371072</td>\n",
       "      <td>-0.215647</td>\n",
       "      <td>2.442506</td>\n",
       "      <td>0.213195</td>\n",
       "      <td>-0.049082</td>\n",
       "      <td>-0.450416</td>\n",
       "      <td>-0.102209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-97.859471</td>\n",
       "      <td>6.989140</td>\n",
       "      <td>1.433912</td>\n",
       "      <td>-8.721733</td>\n",
       "      <td>1.717686</td>\n",
       "      <td>-0.275183</td>\n",
       "      <td>2.668328</td>\n",
       "      <td>0.223085</td>\n",
       "      <td>-0.537166</td>\n",
       "      <td>0.061027</td>\n",
       "      <td>0.281210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-97.858743</td>\n",
       "      <td>6.739877</td>\n",
       "      <td>1.141540</td>\n",
       "      <td>-8.139220</td>\n",
       "      <td>1.388232</td>\n",
       "      <td>-0.450660</td>\n",
       "      <td>1.782010</td>\n",
       "      <td>0.468718</td>\n",
       "      <td>-0.367861</td>\n",
       "      <td>0.356872</td>\n",
       "      <td>-0.287447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-97.862924</td>\n",
       "      <td>3.622315</td>\n",
       "      <td>1.216305</td>\n",
       "      <td>-9.168162</td>\n",
       "      <td>2.304519</td>\n",
       "      <td>-0.233600</td>\n",
       "      <td>3.181717</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>-0.495574</td>\n",
       "      <td>-0.209742</td>\n",
       "      <td>0.336258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-97.862472</td>\n",
       "      <td>3.193638</td>\n",
       "      <td>1.153358</td>\n",
       "      <td>-8.662065</td>\n",
       "      <td>2.059091</td>\n",
       "      <td>-0.212865</td>\n",
       "      <td>1.857561</td>\n",
       "      <td>0.375717</td>\n",
       "      <td>-0.273749</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>-0.082094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-97.852374</td>\n",
       "      <td>5.612611</td>\n",
       "      <td>-0.155147</td>\n",
       "      <td>-1.406663</td>\n",
       "      <td>-0.193498</td>\n",
       "      <td>0.297211</td>\n",
       "      <td>0.908549</td>\n",
       "      <td>-2.039980</td>\n",
       "      <td>0.497813</td>\n",
       "      <td>0.335155</td>\n",
       "      <td>-0.924666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-97.853833</td>\n",
       "      <td>5.947206</td>\n",
       "      <td>0.208716</td>\n",
       "      <td>-2.741957</td>\n",
       "      <td>0.241826</td>\n",
       "      <td>0.169163</td>\n",
       "      <td>1.747724</td>\n",
       "      <td>-2.287369</td>\n",
       "      <td>0.350130</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>-0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-97.857286</td>\n",
       "      <td>2.580381</td>\n",
       "      <td>-0.008892</td>\n",
       "      <td>-3.188387</td>\n",
       "      <td>0.828659</td>\n",
       "      <td>0.210747</td>\n",
       "      <td>2.261113</td>\n",
       "      <td>-2.437732</td>\n",
       "      <td>0.391723</td>\n",
       "      <td>-0.253377</td>\n",
       "      <td>-0.350352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-97.853205</td>\n",
       "      <td>5.668925</td>\n",
       "      <td>-0.474952</td>\n",
       "      <td>-1.602060</td>\n",
       "      <td>0.594994</td>\n",
       "      <td>-2.058870</td>\n",
       "      <td>1.707014</td>\n",
       "      <td>-0.334949</td>\n",
       "      <td>-0.427054</td>\n",
       "      <td>0.048131</td>\n",
       "      <td>-0.853112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-97.853911</td>\n",
       "      <td>5.856318</td>\n",
       "      <td>-0.255935</td>\n",
       "      <td>-2.194026</td>\n",
       "      <td>0.820329</td>\n",
       "      <td>-1.854681</td>\n",
       "      <td>2.601636</td>\n",
       "      <td>-0.617933</td>\n",
       "      <td>-0.647296</td>\n",
       "      <td>-0.073691</td>\n",
       "      <td>-0.215457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-97.862194</td>\n",
       "      <td>2.906355</td>\n",
       "      <td>0.661373</td>\n",
       "      <td>-9.045858</td>\n",
       "      <td>1.858025</td>\n",
       "      <td>1.367925</td>\n",
       "      <td>1.112525</td>\n",
       "      <td>-1.470769</td>\n",
       "      <td>0.515286</td>\n",
       "      <td>-0.035013</td>\n",
       "      <td>-0.284155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-97.856539</td>\n",
       "      <td>9.419318</td>\n",
       "      <td>1.152592</td>\n",
       "      <td>-7.682720</td>\n",
       "      <td>1.155662</td>\n",
       "      <td>-1.606290</td>\n",
       "      <td>2.535045</td>\n",
       "      <td>0.997389</td>\n",
       "      <td>-1.085960</td>\n",
       "      <td>0.288015</td>\n",
       "      <td>-0.023457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-97.856410</td>\n",
       "      <td>9.228584</td>\n",
       "      <td>1.187821</td>\n",
       "      <td>-7.930231</td>\n",
       "      <td>1.151454</td>\n",
       "      <td>-0.386689</td>\n",
       "      <td>2.130944</td>\n",
       "      <td>0.377706</td>\n",
       "      <td>-0.523835</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>-0.072912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-97.850841</td>\n",
       "      <td>8.143350</td>\n",
       "      <td>-0.013671</td>\n",
       "      <td>-1.967127</td>\n",
       "      <td>-0.440974</td>\n",
       "      <td>0.083720</td>\n",
       "      <td>1.137000</td>\n",
       "      <td>-2.123571</td>\n",
       "      <td>0.348750</td>\n",
       "      <td>0.332119</td>\n",
       "      <td>-0.732073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-97.850041</td>\n",
       "      <td>8.101318</td>\n",
       "      <td>-0.108865</td>\n",
       "      <td>-1.197675</td>\n",
       "      <td>-0.430276</td>\n",
       "      <td>0.361182</td>\n",
       "      <td>1.257483</td>\n",
       "      <td>-2.130992</td>\n",
       "      <td>0.341839</td>\n",
       "      <td>0.337657</td>\n",
       "      <td>-0.710130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-97.850919</td>\n",
       "      <td>8.052243</td>\n",
       "      <td>-0.478193</td>\n",
       "      <td>-1.419300</td>\n",
       "      <td>0.136071</td>\n",
       "      <td>-1.940297</td>\n",
       "      <td>1.992169</td>\n",
       "      <td>-0.451756</td>\n",
       "      <td>-0.646528</td>\n",
       "      <td>0.239221</td>\n",
       "      <td>-0.543222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-97.850872</td>\n",
       "      <td>8.157632</td>\n",
       "      <td>-0.428671</td>\n",
       "      <td>-1.393071</td>\n",
       "      <td>0.358217</td>\n",
       "      <td>-1.994899</td>\n",
       "      <td>2.055948</td>\n",
       "      <td>-0.425961</td>\n",
       "      <td>-0.583028</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>-0.638576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-97.745900</td>\n",
       "      <td>12.844747</td>\n",
       "      <td>0.735475</td>\n",
       "      <td>-0.547313</td>\n",
       "      <td>-3.090638</td>\n",
       "      <td>0.333820</td>\n",
       "      <td>2.116723</td>\n",
       "      <td>-1.318813</td>\n",
       "      <td>0.178875</td>\n",
       "      <td>0.349129</td>\n",
       "      <td>0.478519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-97.856198</td>\n",
       "      <td>8.660613</td>\n",
       "      <td>-0.038368</td>\n",
       "      <td>-7.674850</td>\n",
       "      <td>1.809991</td>\n",
       "      <td>-0.867104</td>\n",
       "      <td>2.147998</td>\n",
       "      <td>0.182915</td>\n",
       "      <td>-0.653147</td>\n",
       "      <td>-0.149464</td>\n",
       "      <td>-0.172608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-97.856129</td>\n",
       "      <td>8.704263</td>\n",
       "      <td>-0.062279</td>\n",
       "      <td>-7.658012</td>\n",
       "      <td>1.928893</td>\n",
       "      <td>-0.892891</td>\n",
       "      <td>2.219326</td>\n",
       "      <td>0.169929</td>\n",
       "      <td>-0.641873</td>\n",
       "      <td>-0.162938</td>\n",
       "      <td>-0.198309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1710.822904</td>\n",
       "      <td>1.263764</td>\n",
       "      <td>3.185006</td>\n",
       "      <td>-2.098525</td>\n",
       "      <td>-1.775347</td>\n",
       "      <td>2.915628</td>\n",
       "      <td>3.181134</td>\n",
       "      <td>1.092961</td>\n",
       "      <td>0.933683</td>\n",
       "      <td>-1.946012</td>\n",
       "      <td>-2.538484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-97.837482</td>\n",
       "      <td>27.987130</td>\n",
       "      <td>1.138219</td>\n",
       "      <td>-5.761471</td>\n",
       "      <td>-1.733423</td>\n",
       "      <td>-1.441887</td>\n",
       "      <td>2.038154</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>-1.208983</td>\n",
       "      <td>1.203860</td>\n",
       "      <td>1.101017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-97.862712</td>\n",
       "      <td>3.536812</td>\n",
       "      <td>1.394648</td>\n",
       "      <td>-9.027344</td>\n",
       "      <td>2.046836</td>\n",
       "      <td>-0.378424</td>\n",
       "      <td>2.026379</td>\n",
       "      <td>0.390388</td>\n",
       "      <td>-0.304059</td>\n",
       "      <td>0.030545</td>\n",
       "      <td>0.099678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-97.831114</td>\n",
       "      <td>26.859864</td>\n",
       "      <td>-0.158467</td>\n",
       "      <td>0.971085</td>\n",
       "      <td>-3.315153</td>\n",
       "      <td>-0.694016</td>\n",
       "      <td>1.164692</td>\n",
       "      <td>-1.780013</td>\n",
       "      <td>-0.343309</td>\n",
       "      <td>1.182142</td>\n",
       "      <td>0.463799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-97.837303</td>\n",
       "      <td>27.670829</td>\n",
       "      <td>0.254940</td>\n",
       "      <td>-5.587879</td>\n",
       "      <td>-1.251865</td>\n",
       "      <td>-1.913653</td>\n",
       "      <td>2.138726</td>\n",
       "      <td>0.588986</td>\n",
       "      <td>-1.366438</td>\n",
       "      <td>0.892159</td>\n",
       "      <td>1.019902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-97.862433</td>\n",
       "      <td>3.249528</td>\n",
       "      <td>0.902664</td>\n",
       "      <td>-9.411137</td>\n",
       "      <td>1.845770</td>\n",
       "      <td>1.202366</td>\n",
       "      <td>1.281343</td>\n",
       "      <td>-1.456098</td>\n",
       "      <td>0.484976</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>-0.102383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>-53.910146</td>\n",
       "      <td>-7.478698</td>\n",
       "      <td>-0.314575</td>\n",
       "      <td>2.786956</td>\n",
       "      <td>-0.347455</td>\n",
       "      <td>-1.385600</td>\n",
       "      <td>-0.476866</td>\n",
       "      <td>-0.925190</td>\n",
       "      <td>1.264943</td>\n",
       "      <td>-0.282593</td>\n",
       "      <td>0.245127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>-96.886052</td>\n",
       "      <td>-7.503535</td>\n",
       "      <td>5.275478</td>\n",
       "      <td>0.716871</td>\n",
       "      <td>0.103777</td>\n",
       "      <td>-1.309655</td>\n",
       "      <td>-0.430617</td>\n",
       "      <td>0.210823</td>\n",
       "      <td>-0.449575</td>\n",
       "      <td>1.366320</td>\n",
       "      <td>0.270799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>-98.860855</td>\n",
       "      <td>-6.927254</td>\n",
       "      <td>-0.092354</td>\n",
       "      <td>1.715692</td>\n",
       "      <td>0.118545</td>\n",
       "      <td>0.872171</td>\n",
       "      <td>1.091794</td>\n",
       "      <td>-0.341677</td>\n",
       "      <td>0.380361</td>\n",
       "      <td>-0.262533</td>\n",
       "      <td>0.490952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>-97.863638</td>\n",
       "      <td>-7.106683</td>\n",
       "      <td>-0.545958</td>\n",
       "      <td>-1.074850</td>\n",
       "      <td>1.230316</td>\n",
       "      <td>-0.166552</td>\n",
       "      <td>-0.025912</td>\n",
       "      <td>0.501444</td>\n",
       "      <td>-0.095902</td>\n",
       "      <td>-1.055780</td>\n",
       "      <td>2.318803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>-83.570308</td>\n",
       "      <td>-7.255643</td>\n",
       "      <td>-2.284189</td>\n",
       "      <td>1.743781</td>\n",
       "      <td>0.863020</td>\n",
       "      <td>-1.640673</td>\n",
       "      <td>-0.010022</td>\n",
       "      <td>0.206628</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>-0.511144</td>\n",
       "      <td>-0.699426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>-58.880548</td>\n",
       "      <td>-7.112037</td>\n",
       "      <td>-0.619729</td>\n",
       "      <td>1.821016</td>\n",
       "      <td>0.136637</td>\n",
       "      <td>0.391539</td>\n",
       "      <td>-0.645951</td>\n",
       "      <td>0.277845</td>\n",
       "      <td>-0.103676</td>\n",
       "      <td>0.209438</td>\n",
       "      <td>-0.895355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>586.188140</td>\n",
       "      <td>-6.578308</td>\n",
       "      <td>0.030469</td>\n",
       "      <td>-0.853187</td>\n",
       "      <td>1.873086</td>\n",
       "      <td>2.273332</td>\n",
       "      <td>-1.733270</td>\n",
       "      <td>-2.029864</td>\n",
       "      <td>-0.551654</td>\n",
       "      <td>0.197842</td>\n",
       "      <td>0.077809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>40.139711</td>\n",
       "      <td>-6.748098</td>\n",
       "      <td>-0.686420</td>\n",
       "      <td>1.857612</td>\n",
       "      <td>-0.101463</td>\n",
       "      <td>-0.576935</td>\n",
       "      <td>-0.570406</td>\n",
       "      <td>-0.616728</td>\n",
       "      <td>-0.070004</td>\n",
       "      <td>0.402032</td>\n",
       "      <td>-1.255332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>-88.272202</td>\n",
       "      <td>-7.521014</td>\n",
       "      <td>-0.514204</td>\n",
       "      <td>1.098806</td>\n",
       "      <td>-0.958975</td>\n",
       "      <td>-0.407099</td>\n",
       "      <td>1.007120</td>\n",
       "      <td>-0.260006</td>\n",
       "      <td>-0.329909</td>\n",
       "      <td>0.702998</td>\n",
       "      <td>-1.203732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>-63.912008</td>\n",
       "      <td>-6.323928</td>\n",
       "      <td>-0.236411</td>\n",
       "      <td>-0.074380</td>\n",
       "      <td>0.115819</td>\n",
       "      <td>-0.530048</td>\n",
       "      <td>0.200367</td>\n",
       "      <td>-0.679583</td>\n",
       "      <td>0.385125</td>\n",
       "      <td>0.187740</td>\n",
       "      <td>-0.613642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>-31.360740</td>\n",
       "      <td>-6.537553</td>\n",
       "      <td>-1.157577</td>\n",
       "      <td>1.023714</td>\n",
       "      <td>1.244199</td>\n",
       "      <td>-0.940641</td>\n",
       "      <td>-0.060032</td>\n",
       "      <td>0.608717</td>\n",
       "      <td>-0.651646</td>\n",
       "      <td>0.359834</td>\n",
       "      <td>-0.240703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>135.139016</td>\n",
       "      <td>-7.000231</td>\n",
       "      <td>0.152408</td>\n",
       "      <td>0.993006</td>\n",
       "      <td>0.582270</td>\n",
       "      <td>1.327734</td>\n",
       "      <td>-0.400232</td>\n",
       "      <td>0.177732</td>\n",
       "      <td>0.356423</td>\n",
       "      <td>-0.084169</td>\n",
       "      <td>0.898629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>-96.881885</td>\n",
       "      <td>-6.834011</td>\n",
       "      <td>-0.573209</td>\n",
       "      <td>0.519633</td>\n",
       "      <td>0.228312</td>\n",
       "      <td>-0.363566</td>\n",
       "      <td>0.272809</td>\n",
       "      <td>0.059911</td>\n",
       "      <td>-0.230258</td>\n",
       "      <td>0.113519</td>\n",
       "      <td>0.288144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>-37.771499</td>\n",
       "      <td>-6.612621</td>\n",
       "      <td>-0.334945</td>\n",
       "      <td>0.466736</td>\n",
       "      <td>0.998802</td>\n",
       "      <td>1.678556</td>\n",
       "      <td>0.332621</td>\n",
       "      <td>0.631882</td>\n",
       "      <td>-0.553029</td>\n",
       "      <td>-0.250350</td>\n",
       "      <td>0.710427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>-97.860607</td>\n",
       "      <td>-6.970665</td>\n",
       "      <td>-0.483626</td>\n",
       "      <td>1.773580</td>\n",
       "      <td>-0.897284</td>\n",
       "      <td>1.431871</td>\n",
       "      <td>0.678092</td>\n",
       "      <td>-0.222146</td>\n",
       "      <td>0.331064</td>\n",
       "      <td>-0.710095</td>\n",
       "      <td>1.605711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>36.148649</td>\n",
       "      <td>-6.508711</td>\n",
       "      <td>-0.386991</td>\n",
       "      <td>1.518497</td>\n",
       "      <td>0.715181</td>\n",
       "      <td>-1.132052</td>\n",
       "      <td>0.370738</td>\n",
       "      <td>0.375987</td>\n",
       "      <td>-1.268163</td>\n",
       "      <td>-0.104269</td>\n",
       "      <td>-0.041064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>-48.860041</td>\n",
       "      <td>-6.529447</td>\n",
       "      <td>-1.585138</td>\n",
       "      <td>1.205584</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>-0.013203</td>\n",
       "      <td>-0.457502</td>\n",
       "      <td>0.290197</td>\n",
       "      <td>-0.177197</td>\n",
       "      <td>0.151186</td>\n",
       "      <td>-0.395609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>-89.440876</td>\n",
       "      <td>-7.259754</td>\n",
       "      <td>-1.164316</td>\n",
       "      <td>1.765785</td>\n",
       "      <td>0.577072</td>\n",
       "      <td>-0.571859</td>\n",
       "      <td>0.192683</td>\n",
       "      <td>-0.086895</td>\n",
       "      <td>-0.150150</td>\n",
       "      <td>-0.350300</td>\n",
       "      <td>-0.008367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>-85.382444</td>\n",
       "      <td>-6.724112</td>\n",
       "      <td>-0.215520</td>\n",
       "      <td>0.095988</td>\n",
       "      <td>0.195164</td>\n",
       "      <td>-0.175624</td>\n",
       "      <td>-0.103996</td>\n",
       "      <td>-0.236025</td>\n",
       "      <td>-0.040396</td>\n",
       "      <td>-0.201055</td>\n",
       "      <td>-0.322007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>-55.861626</td>\n",
       "      <td>-7.498360</td>\n",
       "      <td>-1.065231</td>\n",
       "      <td>0.795035</td>\n",
       "      <td>0.192442</td>\n",
       "      <td>-0.148430</td>\n",
       "      <td>-0.758527</td>\n",
       "      <td>-0.153315</td>\n",
       "      <td>0.351319</td>\n",
       "      <td>-0.769232</td>\n",
       "      <td>1.794864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>-91.862581</td>\n",
       "      <td>-7.721095</td>\n",
       "      <td>-0.302012</td>\n",
       "      <td>1.079646</td>\n",
       "      <td>-1.944483</td>\n",
       "      <td>-0.193602</td>\n",
       "      <td>1.881604</td>\n",
       "      <td>-0.978833</td>\n",
       "      <td>0.259207</td>\n",
       "      <td>-0.042112</td>\n",
       "      <td>1.001677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>-68.871079</td>\n",
       "      <td>-7.148461</td>\n",
       "      <td>-1.078937</td>\n",
       "      <td>1.274480</td>\n",
       "      <td>-0.294803</td>\n",
       "      <td>-0.601883</td>\n",
       "      <td>-0.359185</td>\n",
       "      <td>0.487222</td>\n",
       "      <td>-0.427854</td>\n",
       "      <td>0.416656</td>\n",
       "      <td>0.014393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>-66.880761</td>\n",
       "      <td>-7.270176</td>\n",
       "      <td>-0.478725</td>\n",
       "      <td>2.014330</td>\n",
       "      <td>0.227719</td>\n",
       "      <td>-0.415035</td>\n",
       "      <td>-0.365963</td>\n",
       "      <td>-0.402031</td>\n",
       "      <td>0.167167</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>-0.372687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>-11.161051</td>\n",
       "      <td>-7.131235</td>\n",
       "      <td>-0.696405</td>\n",
       "      <td>1.028483</td>\n",
       "      <td>0.204837</td>\n",
       "      <td>-0.127582</td>\n",
       "      <td>-0.890319</td>\n",
       "      <td>-0.207223</td>\n",
       "      <td>0.501197</td>\n",
       "      <td>-0.045773</td>\n",
       "      <td>-0.206577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>-66.451126</td>\n",
       "      <td>-6.813794</td>\n",
       "      <td>-0.704029</td>\n",
       "      <td>0.987974</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.749720</td>\n",
       "      <td>-0.365919</td>\n",
       "      <td>0.195389</td>\n",
       "      <td>0.230602</td>\n",
       "      <td>-0.119063</td>\n",
       "      <td>0.732228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>-69.861626</td>\n",
       "      <td>-6.204368</td>\n",
       "      <td>-0.370423</td>\n",
       "      <td>0.407223</td>\n",
       "      <td>1.884706</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>2.898481</td>\n",
       "      <td>-0.726079</td>\n",
       "      <td>-0.250743</td>\n",
       "      <td>-0.633812</td>\n",
       "      <td>0.367950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>-87.010692</td>\n",
       "      <td>-6.875868</td>\n",
       "      <td>-0.505268</td>\n",
       "      <td>1.976242</td>\n",
       "      <td>1.213577</td>\n",
       "      <td>-0.797068</td>\n",
       "      <td>1.677211</td>\n",
       "      <td>-0.983456</td>\n",
       "      <td>0.562269</td>\n",
       "      <td>-0.518542</td>\n",
       "      <td>0.123108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>-97.871049</td>\n",
       "      <td>-7.062887</td>\n",
       "      <td>-0.203392</td>\n",
       "      <td>1.907777</td>\n",
       "      <td>0.921333</td>\n",
       "      <td>0.843257</td>\n",
       "      <td>1.551943</td>\n",
       "      <td>-0.798074</td>\n",
       "      <td>-0.017035</td>\n",
       "      <td>-0.291351</td>\n",
       "      <td>-0.392511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>-76.862419</td>\n",
       "      <td>-7.437387</td>\n",
       "      <td>-0.697492</td>\n",
       "      <td>0.780136</td>\n",
       "      <td>-0.957925</td>\n",
       "      <td>-0.614511</td>\n",
       "      <td>0.227797</td>\n",
       "      <td>-0.338114</td>\n",
       "      <td>-0.546777</td>\n",
       "      <td>0.674330</td>\n",
       "      <td>-1.608394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>56.137915</td>\n",
       "      <td>-7.813218</td>\n",
       "      <td>-1.602373</td>\n",
       "      <td>0.940629</td>\n",
       "      <td>-0.823088</td>\n",
       "      <td>-1.233230</td>\n",
       "      <td>0.770043</td>\n",
       "      <td>-0.051305</td>\n",
       "      <td>-0.988173</td>\n",
       "      <td>0.820267</td>\n",
       "      <td>-0.991591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1         2          3         4         5   \\\n",
       "0     -98.856112   0.278069 -1.088056  -1.487752  0.771298  1.008360   \n",
       "1     430.136945  -6.091276  0.415409  -1.705227  1.505790  0.512664   \n",
       "2     141.074143   0.689728  0.739069   1.181097 -0.449276  1.799765   \n",
       "3     -39.843731  12.333006  1.114797   3.029670 -3.653070  0.582510   \n",
       "4     -97.871374  -5.085720  1.098542 -10.053107  3.371072 -0.215647   \n",
       "5     -97.859471   6.989140  1.433912  -8.721733  1.717686 -0.275183   \n",
       "6     -97.858743   6.739877  1.141540  -8.139220  1.388232 -0.450660   \n",
       "7     -97.862924   3.622315  1.216305  -9.168162  2.304519 -0.233600   \n",
       "8     -97.862472   3.193638  1.153358  -8.662065  2.059091 -0.212865   \n",
       "9     -97.852374   5.612611 -0.155147  -1.406663 -0.193498  0.297211   \n",
       "10    -97.853833   5.947206  0.208716  -2.741957  0.241826  0.169163   \n",
       "11    -97.857286   2.580381 -0.008892  -3.188387  0.828659  0.210747   \n",
       "12    -97.853205   5.668925 -0.474952  -1.602060  0.594994 -2.058870   \n",
       "13    -97.853911   5.856318 -0.255935  -2.194026  0.820329 -1.854681   \n",
       "14    -97.862194   2.906355  0.661373  -9.045858  1.858025  1.367925   \n",
       "15    -97.856539   9.419318  1.152592  -7.682720  1.155662 -1.606290   \n",
       "16    -97.856410   9.228584  1.187821  -7.930231  1.151454 -0.386689   \n",
       "17    -97.850841   8.143350 -0.013671  -1.967127 -0.440974  0.083720   \n",
       "18    -97.850041   8.101318 -0.108865  -1.197675 -0.430276  0.361182   \n",
       "19    -97.850919   8.052243 -0.478193  -1.419300  0.136071 -1.940297   \n",
       "20    -97.850872   8.157632 -0.428671  -1.393071  0.358217 -1.994899   \n",
       "21    -97.745900  12.844747  0.735475  -0.547313 -3.090638  0.333820   \n",
       "22    -97.856198   8.660613 -0.038368  -7.674850  1.809991 -0.867104   \n",
       "23    -97.856129   8.704263 -0.062279  -7.658012  1.928893 -0.892891   \n",
       "24   1710.822904   1.263764  3.185006  -2.098525 -1.775347  2.915628   \n",
       "25    -97.837482  27.987130  1.138219  -5.761471 -1.733423 -1.441887   \n",
       "26    -97.862712   3.536812  1.394648  -9.027344  2.046836 -0.378424   \n",
       "27    -97.831114  26.859864 -0.158467   0.971085 -3.315153 -0.694016   \n",
       "28    -97.837303  27.670829  0.254940  -5.587879 -1.251865 -1.913653   \n",
       "29    -97.862433   3.249528  0.902664  -9.411137  1.845770  1.202366   \n",
       "..           ...        ...       ...        ...       ...       ...   \n",
       "954   -53.910146  -7.478698 -0.314575   2.786956 -0.347455 -1.385600   \n",
       "955   -96.886052  -7.503535  5.275478   0.716871  0.103777 -1.309655   \n",
       "956   -98.860855  -6.927254 -0.092354   1.715692  0.118545  0.872171   \n",
       "957   -97.863638  -7.106683 -0.545958  -1.074850  1.230316 -0.166552   \n",
       "958   -83.570308  -7.255643 -2.284189   1.743781  0.863020 -1.640673   \n",
       "959   -58.880548  -7.112037 -0.619729   1.821016  0.136637  0.391539   \n",
       "960   586.188140  -6.578308  0.030469  -0.853187  1.873086  2.273332   \n",
       "961    40.139711  -6.748098 -0.686420   1.857612 -0.101463 -0.576935   \n",
       "962   -88.272202  -7.521014 -0.514204   1.098806 -0.958975 -0.407099   \n",
       "963   -63.912008  -6.323928 -0.236411  -0.074380  0.115819 -0.530048   \n",
       "964   -31.360740  -6.537553 -1.157577   1.023714  1.244199 -0.940641   \n",
       "965   135.139016  -7.000231  0.152408   0.993006  0.582270  1.327734   \n",
       "966   -96.881885  -6.834011 -0.573209   0.519633  0.228312 -0.363566   \n",
       "967   -37.771499  -6.612621 -0.334945   0.466736  0.998802  1.678556   \n",
       "968   -97.860607  -6.970665 -0.483626   1.773580 -0.897284  1.431871   \n",
       "969    36.148649  -6.508711 -0.386991   1.518497  0.715181 -1.132052   \n",
       "970   -48.860041  -6.529447 -1.585138   1.205584  0.183320 -0.013203   \n",
       "971   -89.440876  -7.259754 -1.164316   1.765785  0.577072 -0.571859   \n",
       "972   -85.382444  -6.724112 -0.215520   0.095988  0.195164 -0.175624   \n",
       "973   -55.861626  -7.498360 -1.065231   0.795035  0.192442 -0.148430   \n",
       "974   -91.862581  -7.721095 -0.302012   1.079646 -1.944483 -0.193602   \n",
       "975   -68.871079  -7.148461 -1.078937   1.274480 -0.294803 -0.601883   \n",
       "976   -66.880761  -7.270176 -0.478725   2.014330  0.227719 -0.415035   \n",
       "977   -11.161051  -7.131235 -0.696405   1.028483  0.204837 -0.127582   \n",
       "978   -66.451126  -6.813794 -0.704029   0.987974  0.000100 -0.749720   \n",
       "979   -69.861626  -6.204368 -0.370423   0.407223  1.884706  1.395148   \n",
       "980   -87.010692  -6.875868 -0.505268   1.976242  1.213577 -0.797068   \n",
       "981   -97.871049  -7.062887 -0.203392   1.907777  0.921333  0.843257   \n",
       "982   -76.862419  -7.437387 -0.697492   0.780136 -0.957925 -0.614511   \n",
       "983    56.137915  -7.813218 -1.602373   0.940629 -0.823088 -1.233230   \n",
       "\n",
       "           6         7         8         9         10  \n",
       "0   -0.686408  0.491947  0.385315  0.093287 -0.522633  \n",
       "1   -1.236220 -1.386649 -1.130202  1.139248 -0.233847  \n",
       "2    3.680595  0.337945 -1.897309 -0.199442  0.201884  \n",
       "3    3.440398  0.487783 -1.248633  0.690207  0.589026  \n",
       "4    2.442506  0.213195 -0.049082 -0.450416 -0.102209  \n",
       "5    2.668328  0.223085 -0.537166  0.061027  0.281210  \n",
       "6    1.782010  0.468718 -0.367861  0.356872 -0.287447  \n",
       "7    3.181717  0.072721 -0.495574 -0.209742  0.336258  \n",
       "8    1.857561  0.375717 -0.273749  0.011583 -0.082094  \n",
       "9    0.908549 -2.039980  0.497813  0.335155 -0.924666  \n",
       "10   1.747724 -2.287369  0.350130  0.017391 -0.405400  \n",
       "11   2.261113 -2.437732  0.391723 -0.253377 -0.350352  \n",
       "12   1.707014 -0.334949 -0.427054  0.048131 -0.853112  \n",
       "13   2.601636 -0.617933 -0.647296 -0.073691 -0.215457  \n",
       "14   1.112525 -1.470769  0.515286 -0.035013 -0.284155  \n",
       "15   2.535045  0.997389 -1.085960  0.288015 -0.023457  \n",
       "16   2.130944  0.377706 -0.523835  0.359375 -0.072912  \n",
       "17   1.137000 -2.123571  0.348750  0.332119 -0.732073  \n",
       "18   1.257483 -2.130992  0.341839  0.337657 -0.710130  \n",
       "19   1.992169 -0.451756 -0.646528  0.239221 -0.543222  \n",
       "20   2.055948 -0.425961 -0.583028  0.050633 -0.638576  \n",
       "21   2.116723 -1.318813  0.178875  0.349129  0.478519  \n",
       "22   2.147998  0.182915 -0.653147 -0.149464 -0.172608  \n",
       "23   2.219326  0.169929 -0.641873 -0.162938 -0.198309  \n",
       "24   3.181134  1.092961  0.933683 -1.946012 -2.538484  \n",
       "25   2.038154  0.728685 -1.208983  1.203860  1.101017  \n",
       "26   2.026379  0.390388 -0.304059  0.030545  0.099678  \n",
       "27   1.164692 -1.780013 -0.343309  1.182142  0.463799  \n",
       "28   2.138726  0.588986 -1.366438  0.892159  1.019902  \n",
       "29   1.281343 -1.456098  0.484976 -0.016050 -0.102383  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "954 -0.476866 -0.925190  1.264943 -0.282593  0.245127  \n",
       "955 -0.430617  0.210823 -0.449575  1.366320  0.270799  \n",
       "956  1.091794 -0.341677  0.380361 -0.262533  0.490952  \n",
       "957 -0.025912  0.501444 -0.095902 -1.055780  2.318803  \n",
       "958 -0.010022  0.206628  0.006580 -0.511144 -0.699426  \n",
       "959 -0.645951  0.277845 -0.103676  0.209438 -0.895355  \n",
       "960 -1.733270 -2.029864 -0.551654  0.197842  0.077809  \n",
       "961 -0.570406 -0.616728 -0.070004  0.402032 -1.255332  \n",
       "962  1.007120 -0.260006 -0.329909  0.702998 -1.203732  \n",
       "963  0.200367 -0.679583  0.385125  0.187740 -0.613642  \n",
       "964 -0.060032  0.608717 -0.651646  0.359834 -0.240703  \n",
       "965 -0.400232  0.177732  0.356423 -0.084169  0.898629  \n",
       "966  0.272809  0.059911 -0.230258  0.113519  0.288144  \n",
       "967  0.332621  0.631882 -0.553029 -0.250350  0.710427  \n",
       "968  0.678092 -0.222146  0.331064 -0.710095  1.605711  \n",
       "969  0.370738  0.375987 -1.268163 -0.104269 -0.041064  \n",
       "970 -0.457502  0.290197 -0.177197  0.151186 -0.395609  \n",
       "971  0.192683 -0.086895 -0.150150 -0.350300 -0.008367  \n",
       "972 -0.103996 -0.236025 -0.040396 -0.201055 -0.322007  \n",
       "973 -0.758527 -0.153315  0.351319 -0.769232  1.794864  \n",
       "974  1.881604 -0.978833  0.259207 -0.042112  1.001677  \n",
       "975 -0.359185  0.487222 -0.427854  0.416656  0.014393  \n",
       "976 -0.365963 -0.402031  0.167167  0.076289 -0.372687  \n",
       "977 -0.890319 -0.207223  0.501197 -0.045773 -0.206577  \n",
       "978 -0.365919  0.195389  0.230602 -0.119063  0.732228  \n",
       "979  2.898481 -0.726079 -0.250743 -0.633812  0.367950  \n",
       "980  1.677211 -0.983456  0.562269 -0.518542  0.123108  \n",
       "981  1.551943 -0.798074 -0.017035 -0.291351 -0.392511  \n",
       "982  0.227797 -0.338114 -0.546777  0.674330 -1.608394  \n",
       "983  0.770043 -0.051305 -0.988173  0.820267 -0.991591  \n",
       "\n",
       "[984 rows x 11 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# COPY\n",
    "# XGBoost Model Trainer (custom decorator function)\n",
    "\n",
    "def XGBoost_Trainerrrr(df, target=None, model= 'other', resampling=None,\n",
    "                    booster=\"gbtree\", objective=\"reg:linear\",\n",
    "                    test_size=0.2,\n",
    "                    random_state=69,\n",
    "                    normalization=True,\n",
    "                    pca=False,\n",
    "                    hyperparameter_test= \"none\",\n",
    "                    cv_metric=\"rmse\",\n",
    "                    missing_values=\"NaN\",\n",
    "                    imputation_strategy=\"mean\",\n",
    "                    n_iter= 25,\n",
    "                    hypertune_folds= 5,\n",
    "                    cv_folds= 5,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    grid_params = \"preset\",\n",
    "                    random_params = \"preset\"):\n",
    "    \"\"\"This function requires the xgboost package to be imported as xgb.\"\"\"\n",
    "    \n",
    "    if model == \"Tree Regressor\":\n",
    "        bj = ['gbtree','reg:linear']\n",
    "    elif model == \"Linear Regressor\":\n",
    "        bj = ['gblinear','reg:linear']\n",
    "    elif model == \"Tree Classifier\":\n",
    "        bj = ['gbtree', 'reg:logistic']\n",
    "    elif model == \"Linear Classifier\":\n",
    "        bj = ['gblinear', 'reg:logistic']\n",
    "    elif mode == 'other':\n",
    "        bj = [booster, objective]\n",
    "    \n",
    "    grid_parameters = {}\n",
    "    random_parameters = {}\n",
    "    \n",
    "    if grid_params == \"preset\":\n",
    "        grid_params = grid_parameters\n",
    "    if random_params == \"preset\":\n",
    "        random_params == random_parameters\n",
    "    \n",
    "    regression_objectives=[]\n",
    "    classification_objectives = [\"reg:logistic\"]\n",
    "\n",
    "    # Data Preprocessing Pipeline Steps\n",
    "    preprocessing_steps = [\n",
    "            (\"Imputer\", Imputer(missing_values=missing_values, strategy= imputation_strategy, axis=0))\n",
    "                            ]\n",
    "    \n",
    "    # Scaling\n",
    "    if normalization == True:\n",
    "        preprocessing_steps = preprocessing_steps + [\n",
    "            (\"Scaler\", StandardScaler())\n",
    "                            ]\n",
    "    \n",
    "    # Dummy Variable Creation\n",
    "    class Dictionator:\n",
    "        def __init__(self, orient=\"records\"):\n",
    "            self.orient = orient\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):\n",
    "            return X.to_dict(self.orient), y.to_dict(self.orient)\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "    \n",
    "    \n",
    "    preprocessing_steps = preprocessing_steps + [ \n",
    "        (\"Encoder\", FeatureUnion(\n",
    "            [(\"Label\",LabelEncoder()),\n",
    "             (\"Hot\", OneHotEncoder())]))\n",
    "                            ]  \n",
    "    \n",
    "    #### define a CLASS for undersampling\n",
    "    class Binary_Undersampler:\n",
    "        \"\"\"This class is applicable only on data for binary classification.\n",
    "        This class assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X, y):\n",
    "            y = y.values.ravel()\n",
    "            self.Positive_count = y.values.sum()\n",
    "            self.Positive_indices = np.array(y[y.iloc[:,-1] ==1].index)\n",
    "            self.Negative_indices = y[y.iloc[:,-1] == 0].index\n",
    "            self.random_Negative_indices = np.array(np.random.choice(self.Negative_indices, self.Positive_count,\n",
    "                                                                     replace = False))\n",
    "            self.Undersample_indices = np.concatenate([self.Positive_indices, self.random_Negative_indices])\n",
    "            return X.loc[self.Undersample_indices,:], y.loc[self.Undersample_indices,:]\n",
    "        \n",
    "        def fit_transform(self, X, y):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)\n",
    "\n",
    "    \n",
    "    # Resampling Method\n",
    "    if objective in classification_objectives or model in [\"Tree Classifier\",\"Linear Classifier\"]:\n",
    "        if resampling == \"Undersampling\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Undersampler\", Binary_Undersampler())]\n",
    "        elif resampling == \"SMOTE\":\n",
    "            preprocessing_steps = preprocessing_steps + [(\"The Oversampler\", SMOTE())]\n",
    "            \n",
    "    # Principal Component Analysis (PCA)\n",
    "    if pca == True:\n",
    "        preprocessing_steps = preprocessing_steps + [(\"PCA\", PCA())]\n",
    "    \n",
    "    # Separating features and target\n",
    "    X, y = df[df.columns.tolist()[:,-1]], df[df.columns.tolist()[-1]]\n",
    "    \n",
    "    # Split between test and training sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    \n",
    "    # Completing the pipeline with the appropriat XGBoost Model\n",
    "    # correct version\n",
    "    if bj[0] == \"gblinear\":\n",
    "        DM_train = xgb.DMAtrix(data=X_train, label=y_train)\n",
    "        if hyperparameter_test == \"Grid Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", GridSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                      param_grid= grid_params,\n",
    "                                      scoring= scoring,\n",
    "                                      cv= hypertune_folds,\n",
    "                                      verbose=1))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"Random Search\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Tuner\", RandomizedSearchCV(estimator=xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]),\n",
    "                                                                param_distributions=random_parameters,\n",
    "                                                                scoring= scoring,\n",
    "                                                                cv = hypertune_folds,\n",
    "                                                                n_iter= n_iter))\n",
    "                        ]\n",
    "        elif hyperparameter_test == \"none\":\n",
    "            pipeline = Pipeline[\n",
    "                preprocessing_steps,\n",
    "                (\"Model\", xgb.train(dtrain=DM_train, booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "    \n",
    "    elif bj[0] == \"gbtree\":\n",
    "        if bj[1] in regression_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBRegressor(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBRegressor(booster=bj[0], objective=bj[1]))\n",
    "                        ]\n",
    "\n",
    "        elif bj[1] in classification_objectives:\n",
    "            if hyperparameter_test == \"Grid Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", GridSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                          param_grid= grid_params,\n",
    "                                          scoring= scoring,\n",
    "                                          cv= hypertune_folds,\n",
    "                                          verbose=1))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"Random Search\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Tuner\", RandomizedSearchCV(estimator=xgb.XGBClassifier(booster=bj[0], objective=bj[1]),\n",
    "                                                                           param_distributions=random_parameters,\n",
    "                                                                           scoring= scoring,\n",
    "                                                                           cv = hypertune_folds,\n",
    "                                                                           n_iter= n_iter))\n",
    "                        ]\n",
    "            elif hyperparameter_test == \"none\":\n",
    "                pipeline = Pipeline[\n",
    "                    preprocessing_steps,\n",
    "                    (\"Model\", xgb.XGBClassifier(booster=bj[0], objective=bj[1]))\n",
    "                        ]             \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fitting the pipeline \n",
    "    pipeline.fit(X_train.to_dict(\"records\"), y_train)\n",
    "    \n",
    "    if hyperparameter_test == \"Grid Search\" or hyperparameter_test == \"Random Search\":\n",
    "        if bj[0] == \"gblinear\":\n",
    "            xgb_model = xgb.XGBRegressor(booster=\"gbtree\", objective=\"reg:linear\", **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Linear Regressor\":\n",
    "            xgb_model == xgb.train(booster=\"gblinear\", objective='reg:linear', **pipeline.best_params_)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "        elif model == \"Tree Classifier\":\n",
    "            xgb_model == XGBClassifier(booster=\"gbtree\")\n",
    "\n",
    "            \n",
    "        return pd.concat([pd.DataFrame(Imputer(missing_values= self.missing_values,\\\n",
    "                                                           strategy=self.num_imput_strat,\\\n",
    "                                   axis=self.imputation_axis).fit_transform(X.drop(df.columns[[-1,]], axis=1)\\\n",
    "                                                    .drop(X.index[pd.isnull(X.iloc[:,-1]).any(1).nonzero()[0]]))), \\\n",
    "                                pd.DataFrame(X.iloc[:,-1]).dropna()], axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
